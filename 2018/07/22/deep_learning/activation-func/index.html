<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="常见激活函数"><meta name="keywords" content="神经网络,激活函数"><meta name="author" content="NYY,undefined"><meta name="copyright" content="NYY"><title>常见激活函数 | NYY's blog</title><link rel="shortcut icon" href="/img/my_icon.jpg"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#神经网络常用激活函数"><span class="toc-number">1.</span> <span class="toc-text">神经网络常用激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#为什么需要激活函数"><span class="toc-number">1.1.</span> <span class="toc-text">为什么需要激活函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#常见的激活函数"><span class="toc-number">1.2.</span> <span class="toc-text">常见的激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sigmoid函数"><span class="toc-number">1.2.1.</span> <span class="toc-text">Sigmoid函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#缺点："><span class="toc-number">1.2.1.1.</span> <span class="toc-text">缺点：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tanh函数"><span class="toc-number">1.2.2.</span> <span class="toc-text">Tanh函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#优点："><span class="toc-number">1.2.2.1.</span> <span class="toc-text">优点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#缺点：-1"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">缺点：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLu函数系列："><span class="toc-number">1.2.3.</span> <span class="toc-text">ReLu函数系列：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ReLu"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">ReLu</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#优点：-1"><span class="toc-number">1.2.3.1.1.</span> <span class="toc-text">优点：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#缺点：-2"><span class="toc-number">1.2.3.1.2.</span> <span class="toc-text">缺点：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Leakly-ReLu"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">Leakly ReLu</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Parametric-ReLU-P-ReLu"><span class="toc-number">1.2.3.3.</span> <span class="toc-text">Parametric ReLU(P-ReLu)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Randomized-ReLU-R-ReLu"><span class="toc-number">1.2.3.4.</span> <span class="toc-text">Randomized ReLU (R-ReLu)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Maxout"><span class="toc-number">1.2.4.</span> <span class="toc-text">Maxout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Softmax"><span class="toc-number">1.2.5.</span> <span class="toc-text">Softmax</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">NYY</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/niuyuanyuanna" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">47</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">36</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">10</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://www.ouyangsong.com" target="_blank">欧阳松的博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://github.com/niuyuanyuanna/BlogImages/raw/master/background/deep_learning.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">NYY's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">常见激活函数</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-07-22</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Deep-Learning/">Deep Learning</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/2018/07/22/deep_learning/activation-func/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2018/07/22/deep_learning/activation-func/"></span></a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1,548</span><span class="post-meta__separator">|</span><span>Reading time: 5 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="神经网络常用激活函数"><a href="#神经网络常用激活函数" class="headerlink" title="神经网络常用激活函数"></a><center>神经网络常用激活函数</center></h1><h2 id="为什么需要激活函数"><a href="#为什么需要激活函数" class="headerlink" title="为什么需要激活函数"></a>为什么需要激活函数</h2><p>激活函数的性质：</p>
<ul>
<li><strong>非线性</strong>：激活函数为线性函数时，两层神经网络就可以拟合所有的线性函数。若激活函数为恒等激活函数，即$f(x)=x$时，不满足条件</li>
<li><strong>可微性</strong>：当优化方法是基于梯度的时候，必须满足可微性</li>
<li><strong>单调性</strong>：激活函数是单调函数时，单层网络才能保证是凸函数</li>
<li><strong>$f(x)\approx x $</strong>：当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。</li>
<li><strong>输出值的范围</strong>：激活函数输出值的范围是有限的时候，基于梯度的优化方法更加稳定，因为特征的表示受有限权值的影响更显著；激活函数范围无限时，模型训练更高效，需要更小的learning rate。</li>
</ul>
<h2 id="常见的激活函数"><a href="#常见的激活函数" class="headerlink" title="常见的激活函数"></a>常见的激活函数</h2><h3 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h3><p>Sigmoid又叫作 Logistic 激活函数，它将实数值压缩进 0 到 1 的区间内，还可以在预测概率的输出层中使用。该函数将大的负数转换成 0，将大的正数转换成 1。数学公式为：<br>$$<br>\sigma(x) = \frac{1}{1+e^{-x}}<br>$$</p>
<p>它是关于中心点（0, 0.5）对称的函数，图形为：</p>
<center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/deepLearning/37147146.jpg" alt="sigmoid function" title="Sigmoid function" width="70%/"><br></center>

<p>Sigmoid函数导数为：<br>$$<br>\sigma{}’(x)=\sigma (x)(1-\sigma (x))<br>$$<br>图形为：</p>
<center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/deepLearning/99194005.jpg" alt="sigmoid导数" title="Sigmoid function 导数" width="75%/"><br></center>

<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><ul>
<li>会有梯度弥散。Sigmoids saturate and kill gradients. sigmoid 有一个非常致命的缺点，当输入非常大或者非常小的时候（saturation），这些神经元的梯度是接近于0的，从图中可以看出梯度的趋势。需要尤其注意参数的初始值来尽量避免saturation的情况。如果初始值很大，大部分神经元可能都会处在saturation的状态而把gradient kill掉，这会导致网络变的很难学习。</li>
<li>不关于原点对称。 output 不是0均值。这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。  产生的一个结果就是：如果数据进入神经元的时候是正的(e.g. $x&gt;0$，elementwise in$f=w^{T}x + b$ )，那么$w$计算出的梯度也会始终都是正的。 如果输入神经元的数据总是正数，那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数，这将会导致梯度下降权重更新时出现z字型的下降。</li>
<li>计算指数耗时。</li>
</ul>
<h3 id="Tanh函数"><a href="#Tanh函数" class="headerlink" title="Tanh函数"></a>Tanh函数</h3><p>tanh和sigmoid函数相似，实际上，tanh 是sigmoid的变形：<br>$$<br>tanh(x) = 2sigmoid(2x) -1<br>$$<br>与 sigmoid 不同的是，tanh 是0均值的。它的图像关于原点中心对称。因此，实际应用中，tanh 会比 sigmoid 更好。</p>
<center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/deepLearning/91091273.jpg" alt="tanh function" title="tant function" width="70%/"><br></center>

<h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h4><ul>
<li>解决原点对称的问题。tanh解决了Sigmoid的输出是不是零中心的问题。</li>
<li>收敛速度比sigmoid更快</li>
</ul>
<h4 id="缺点：-1"><a href="#缺点：-1" class="headerlink" title="缺点："></a>缺点：</h4><ul>
<li>仍然没有解决梯度弥散问题</li>
</ul>
<h3 id="ReLu函数系列："><a href="#ReLu函数系列：" class="headerlink" title="ReLu函数系列："></a>ReLu函数系列：</h3><h4 id="ReLu"><a href="#ReLu" class="headerlink" title="ReLu"></a>ReLu</h4><p>ReLU非线性函数图像如下图所示。相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用；sigmoid和tanh在求导时含有指数运算，而ReLU求导几乎不存在任何计算量。<br>其公式为：<br>$$<br>f(x) = max(0,x)<br>$$</p>
<center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/deepLearning/48089832.jpg" alt="ReLu function" title="ReLu function" width="70%/"><br></center>

<h5 id="优点：-1"><a href="#优点：-1" class="headerlink" title="优点："></a>优点：</h5><ul>
<li>单侧抑制； </li>
<li>相对宽阔的兴奋边界； </li>
<li>稀疏激活性。 </li>
</ul>
<h5 id="缺点：-2"><a href="#缺点：-2" class="headerlink" title="缺点："></a>缺点：</h5><ul>
<li>ReLU单元比较脆弱并且可能“死掉”，而且是不可逆的，因此导致了数据多样化的丢失。通过合理设置学习率，会降低神经元“死掉”的概率。 </li>
</ul>
<h4 id="Leakly-ReLu"><a href="#Leakly-ReLu" class="headerlink" title="Leakly ReLu"></a>Leakly ReLu</h4><p>Leaky ReLUs用来解决 “dying ReLU” 问题。与 ReLU 不同的是：<br>$$<br>f(x)=\left{\begin{matrix}<br>\alpha x &amp;  (x&lt;0)\<br> x &amp; (x&gt;=0)<br>\end{matrix}\right.<br>$$<br>此处$\alpha $是一个较小的常数，这样既修正了数据分布，又保留了负半轴的值，使得负半轴的信息不会全部丢失。<br>其图像为：</p>
<center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/deepLearning/50011864.jpg" alt="Leakly ReLu function" title="Leakly ReLu function" width="50%/"><br></center>

<h4 id="Parametric-ReLU-P-ReLu"><a href="#Parametric-ReLU-P-ReLu" class="headerlink" title="Parametric ReLU(P-ReLu)"></a>Parametric ReLU(P-ReLu)</h4><p>对于 Leaky ReLU 中的$\alpha$，通常都是通过先验知识人工赋值的。 然而可以观察到，损失函数对α的导数我们是可以求得的，可以将它作为一个参数进行训练，而且效果更好。<br>对$\alpha $的导数如下：<br>$$<br>\frac{\delta y_{i}}{\delta \alpha} = \left{\begin{matrix}<br>0 &amp; y_{i} &gt; 0 \<br> y_{i}&amp; other<br>\end{matrix}\right.<br>$$</p>
<h4 id="Randomized-ReLU-R-ReLu"><a href="#Randomized-ReLU-R-ReLu" class="headerlink" title="Randomized ReLU (R-ReLu)"></a>Randomized ReLU (R-ReLu)</h4><p>Randomized Leaky ReLU是 leaky ReLU 的random 版本 （$\alpha$ 是随机的）。核心思想就是，在训练过程中，$\alpha$ 是一个服从高斯分布的 $U(l,u)$ 中 随机抽取出来的，然后在测试过程中进行修正（有点像dropout的用法）。<br>数学表达式为：<br>$$<br>y_{j,i}=\left{\begin{matrix}<br>x_{j,i} &amp;x_{j,i}\geq 0 \<br>a_{j,i}x_{j,i} &amp; x_{j,i}&lt;0<br>\end{matrix}\right.<br>$$</p>
<p>其中$a_{j,i}\sim U(l,u)$，$l&lt;u$且$u\in [0,1)$。在测试阶段，把训练过程中所有的$\alpha_{j,i}$取个平均值 。</p>
<p>测试阶段激活函数为：<br>$$<br>y_{ij}=\frac{x_{ij}}{\frac {l+u}{2}}<br>$$</p>
<center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/deepLearning/21922024.jpg" alt="P-ReLu and R-ReLu" title="P-ReLu and R-ReLu" width="100%/"><br></center>

<h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><p>Maxout是对ReLU和leaky ReLU的一般化归纳，假设$w$是二维的，函数公式为：<br>$$<br>f(x) = max(w_{1}^{T}x+b_{1},w_{2}^{T}x+b_{2})<br>$$</p>
<p>Maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。作者从数学的角度上也证明了这个结论，即只需2个maxout节点就可以拟合任意的凸函数了（相减），前提是”隐隐含层”节点的个数可以任意多。</p>
<p>Maxout非线性函数图像如下图所示。Maxout具有ReLU的优点，如计算简单，不会 saturation，同时又没有ReLU的一些缺点，如容易死掉。 </p>
<center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/deepLearning/43488930.jpg" alt="Maxout" title="Maxout" width="100%/"><br></center>

<p>由公式可以看出每个神经元的参数都变为原来的两倍，导致整体参数增加。</p>
<h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p>Softmax用于多分类神经网络输出，目的是让大的更大。函数公式是：<br>$$<br>\sigma(z)<em>{j}=\frac{e^{z</em>{j}}}{ \sum_{k=1}^{K} e^{z_{k}}}<br>$$</p>
<center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/deepLearning/90715034.jpg" alt="Softmax" title="Softmax" width="75%"><br></center>

<p>Softmax是Sigmoid的扩展，当类别数k＝2时，Softmax回归退化为Logistic回归。  </p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">NYY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2018/07/22/deep_learning/activation-func/">http://yoursite.com/2018/07/22/deep_learning/activation-func/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/神经网络/">神经网络</a><a class="post-meta__tags" href="/tags/激活函数/">激活函数</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/07/22/computer_version/imgSegmentation/"><i class="fa fa-chevron-left">  </i><span>图像分割</span></a></div><div class="next-post pull-right"><a href="/2018/07/19/offer_problem/sort-summery/"><span>常见的排序算法</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://yoursite.com/2018/07/22/deep_learning/activation-func/';
  this.page.identifier = '2018/07/22/deep_learning/activation-func/';
  this.page.title = '常见激活函数';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'niuyuanyuan' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-src" src="https://niuyuanyuan.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By NYY</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>