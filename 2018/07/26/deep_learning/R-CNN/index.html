<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="R-CNN目标检测方法"><meta name="keywords" content="CV,object detection"><meta name="author" content="NYY,undefined"><meta name="copyright" content="NYY"><title>R-CNN目标检测方法 | NYY's blog</title><link rel="shortcut icon" href="/img/my_icon.jpg"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#物体检测R-CNN介绍"><span class="toc-number">1.</span> <span class="toc-text">物体检测R-CNN介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#R-CNN"><span class="toc-number">1.1.</span> <span class="toc-text">R-CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#检测过程"><span class="toc-number">1.1.1.</span> <span class="toc-text">检测过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Selective-Search"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">Selective Search</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#候选区"><span class="toc-number">1.1.1.1.1.</span> <span class="toc-text">候选区</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#合并规则（优先合并以下四种区域）"><span class="toc-number">1.1.1.1.2.</span> <span class="toc-text">合并规则（优先合并以下四种区域）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#多样化和后处理"><span class="toc-number">1.1.1.1.3.</span> <span class="toc-text">多样化和后处理</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#特征提取"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">特征提取</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#预处理"><span class="toc-number">1.1.1.2.1.</span> <span class="toc-text">预处理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#CNN"><span class="toc-number">1.1.1.2.2.</span> <span class="toc-text">CNN</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#分类"><span class="toc-number">1.1.1.2.3.</span> <span class="toc-text">分类</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#regression"><span class="toc-number">1.1.1.2.4.</span> <span class="toc-text">regression</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#优缺点分析"><span class="toc-number">1.1.2.</span> <span class="toc-text">优缺点分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#优点"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#缺点"><span class="toc-number">1.1.2.2.</span> <span class="toc-text">缺点</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">NYY</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/niuyuanyuanna" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">28</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">21</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">5</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://www.ouyangsong.com" target="_blank">欧阳松的博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(http://p6um59a45.bkt.clouddn.com/18-7-26/41168657.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">NYY's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">R-CNN目标检测方法</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-07-26</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Deep-Learning/">Deep Learning</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/2018/07/26/deep_learning/R-CNN/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2018/07/26/deep_learning/R-CNN/"></span></a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1,491</span><span class="post-meta__separator">|</span><span>Reading time: 5 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="物体检测R-CNN介绍"><a href="#物体检测R-CNN介绍" class="headerlink" title="物体检测R-CNN介绍"></a>物体检测R-CNN介绍</h1><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p>R-CNN的贡献主要是第一次将CNN引入到目标检测中，主要解决两个问题：</p>
<ul>
<li>速度：经典的目标检测算法使用滑动窗法依次判断所有可能的区域。R-CNN则预先提取一系列较可能是物体的候选区域，之后仅在这些候选区域上提取特征，进行判断。</li>
<li>训练集：经典的目标检测算法在区域中提取人工设定的特征（Haar，HOG）。R-CNN则需要训练CNN进行特征提取。可供使用的有两个数据库：<ul>
<li>一个较大的识别库（ImageNet ILSVC 2012）：标定每张图片中物体的类别。一千万图像，1000类。 </li>
<li>一个较小的检测库（PASCAL VOC 2007）：标定每张图片中，物体的类别和位置。一万图像，20类。</li>
</ul>
</li>
</ul>
<h3 id="检测过程"><a href="#检测过程" class="headerlink" title="检测过程"></a>检测过程</h3><p>R-CNN的检测过程主要分为四个步骤：</p>
<ul>
<li>使用<strong>Selective Search</strong>方法，每张图片生成1K~2K个RoI；</li>
<li>对每个RoI使用<strong>CNN</strong>提取特征；</li>
<li>将feature vector 送入<strong>SVM</strong>二分类器中，判断是否属于该类；</li>
<li>bounding-box regression精细修正RoI位置。</li>
</ul>
<p>其整体识别流程如下图所示：<br><img src="http://p6um59a45.bkt.clouddn.com/18-7-26/83769262.jpg" alt="R-CNN" title="R-CNN" width="75%/"></p>
<h4 id="Selective-Search"><a href="#Selective-Search" class="headerlink" title="Selective Search"></a>Selective Search</h4><p>选择性搜索是一种用于目标检测的区域推荐算法。它的设计速度快，召回率高。它是根据颜色、纹理、大小和形状的兼容性，计算相似区域的层次分组。 </p>
<h5 id="候选区"><a href="#候选区" class="headerlink" title="候选区"></a>候选区</h5><ul>
<li>使用一种分割手段，将图像分割成小区域 </li>
<li>查看现有小区域，合并可能性最高的两个区域。重复直到整张图像合并成一个区域位置 </li>
<li>输出所有曾经存在过的区域，所谓候选区域 </li>
<li>候选区域生成和后续步骤相对独立，实际可以使用任意算法进行。</li>
</ul>
<h5 id="合并规则（优先合并以下四种区域）"><a href="#合并规则（优先合并以下四种区域）" class="headerlink" title="合并规则（优先合并以下四种区域）"></a>合并规则（优先合并以下四种区域）</h5><ul>
<li>颜色（颜色直方图）相近的 </li>
<li>纹理（梯度直方图）相近的 </li>
<li>合并后总面积小的 </li>
<li>合并后，总面积在其BBOX中所占比例大的</li>
</ul>
<h5 id="多样化和后处理"><a href="#多样化和后处理" class="headerlink" title="多样化和后处理"></a>多样化和后处理</h5><p>为尽可能不遗漏候选区域，上述操作在多个颜色空间中同时进行（RGB,HSV,Lab等）。在一个颜色空间中，使用上述四条规则的不同组合进行合并。所有颜色空间与所有规则的全部结果，在去除重复后，都作为候选区域输出。</p>
<h4 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h4><h5 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h5><p>将每个region都wrap到固定的大小（227×227） 。此处scale时需要注意：外扩的尺寸大小，形变时是否保持原比例，对框外区域直接截取还是补灰 。</p>
<h5 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h5><p>将所有变形后的region输入到CNN中，网络结构为Hinton 2012年在Image Net上的分类网络AlexNet，其网络结构为：<br><img src="http://p6um59a45.bkt.clouddn.com/18-7-26/66806389.jpg" alt="AlexNet" title="AlexNet" width="75%"></p>
<p>AlexNet使用了两块GPU训练神经网络。此网络提取的特征为4096维，之后送入一个4096-&gt;1000的全连接(fc)层进行分类。  学习率0.01。<br>训练的过程分为两步：</p>
<ul>
<li><strong>pre-training</strong> 使用ILSVRC 2012的数据集进行有监督的分类训练（迁移训练），初步训练出CNN每层的参数，输出1000维类别标号；</li>
<li><strong>fine-tuning</strong> 使用PASCAL VOC2007数据集，将第2步训练出来的CNN模型替换最后一层，变为N+1个输出，采用SGD方法训练最后一层参数。</li>
</ul>
<h5 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h5><p>对每一类目标，使用一个线性SVM二类分类器进行判别。输入为深度网络输出的4096维特征，输出是否属于此类。<br>由于负样本很多，使用hard negative mining方法。 样本判别方法为：</p>
<ul>
<li>正样本：本类的真实值</li>
<li>负样本：考察每一个候选框，如果和本类所有标定框的重叠都小于0.3，认定其为负样本。</li>
</ul>
<h5 id="regression"><a href="#regression" class="headerlink" title="regression"></a>regression</h5><ol>
<li>目标检测问题的识别精度为：<strong>mAP</strong>（mean average precision）</li>
</ol>
<ul>
<li><strong>precision</strong>：对于某张图片计算object C在图片上的查准率<ul>
<li>某图片上C识别正确的个数/ 某图片C的总个数</li>
<li>$precision_{c} = N(TruePositives)<em>{c} / N(TotalObjects)</em>{c}$</li>
</ul>
</li>
<li><strong>average precision</strong>：对于object C在多张图片上的查准率<ul>
<li>每张图片的precisionc的和 / 含有object C的图片的数目</li>
<li>$averagePrecision_{c} = sum(precision_{c}) / N(TotalImages)_{c}$</li>
</ul>
</li>
<li><strong>mean average precision</strong>:对整个数据集的多个object的平均查准率<ul>
<li>每个objectc的average precision / 总的object数目</li>
<li>$meanAveragePrecision = sum(averagePrecision_{c}) / N(classcs)$</li>
</ul>
</li>
</ul>
<ol start="2">
<li>目标检测定位精度：<strong>IoU</strong>（intersection over union ）</li>
</ol>
<ul>
<li>检测结果（detection result）和真实值（ground truth）的交集 / 其并集</li>
<li>IoU = (GT∩DR) / (GT∪DR) </li>
</ul>
<ol start="3">
<li>机器学习中的评价指标：<strong>accuracy、precision、recall </strong></li>
</ol>
<ul>
<li>TP（true positive）正样本中预测为正样本        正确预测为正样本</li>
<li>FP（false positive）负样本中预测为正样本       错误预测为正样本</li>
<li>FN（false negative）正样本中预测为负样本     错误预测为负样本</li>
<li>TN（true negative）负样本中预测为负样本      正确预测为负样本 </li>
</ul>
<p>accuracy：预测正确的样本数                                       accuracy = (TP + TN) / (TP + FP + FN + TN) </p>
<p>precision：预测为正的样本中，正确预测的比例        precision = TP / (TP + FP) </p>
<p>recall：正样本中，正确预测的比例                              recall = TP / (TP + FN)   </p>
<p>F1: precision和recall的综合指标                                  F1 = 2×(precision / (precision + recall)) </p>
<ol start="4">
<li>回归器</li>
</ol>
<p>在R-CNN中，对每一类目标，使用一个线性脊回归器进行精修。正则项$\lambda=10000$。  输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移。  判定为本类的候选框中，和真值重叠面积大于0.6的候选框为正样本。</p>
<h3 id="优缺点分析"><a href="#优缺点分析" class="headerlink" title="优缺点分析"></a>优缺点分析</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li>首次使用CNN进行特征提取</li>
<li>使用bounding box regression进行目标包围框的修改 </li>
</ul>
<h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>selective search算法较为耗时，每帧图像需要2s处理时间</li>
<li>CNN前向传播较为耗时，对每一个RoI（region of interest）都需要输入AlexNet提取特征</li>
<li>三个步骤分别训练，消耗存储空间 </li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">NYY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2018/07/26/deep_learning/R-CNN/">http://yoursite.com/2018/07/26/deep_learning/R-CNN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/CV/">CV</a><a class="post-meta__tags" href="/tags/object-detection/">object detection</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/07/26/deep_learning/fast-R-CNN/"><i class="fa fa-chevron-left">  </i><span>R-CNN</span></a></div><div class="next-post pull-right"><a href="/2018/07/22/deep_learning/imgSegmentation/"><span>图像分割</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://yoursite.com/2018/07/26/deep_learning/R-CNN/';
  this.page.identifier = '2018/07/26/deep_learning/R-CNN/';
  this.page.title = 'R-CNN目标检测方法';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'niuyuanyuan' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-src" src="https://niuyuanyuan.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 By NYY</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>