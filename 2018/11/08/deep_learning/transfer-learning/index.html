<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="迁移学习--样本自适应"><meta name="keywords" content="CV,transfer learning"><meta name="author" content="NYY,undefined"><meta name="copyright" content="NYY"><title>迁移学习--样本自适应 | NYY's blog</title><link rel="shortcut icon" href="/img/my_icon.jpg"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#迁移学习——样本自适应"><span class="toc-number">1.</span> <span class="toc-text">迁移学习——样本自适应</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#DaNN"><span class="toc-number">1.1.</span> <span class="toc-text">DaNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#网络结构"><span class="toc-number">1.1.1.</span> <span class="toc-text">网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#创新点"><span class="toc-number">1.1.2.</span> <span class="toc-text">创新点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#缺点"><span class="toc-number">1.1.3.</span> <span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MMD"><span class="toc-number">1.1.4.</span> <span class="toc-text">MMD</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DDC-Deep-Domain-Confusion"><span class="toc-number">1.2.</span> <span class="toc-text">DDC(Deep Domain Confusion)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DAN-Deep-Adaptation-Networks"><span class="toc-number">1.3.</span> <span class="toc-text">DAN(Deep Adaptation Networks)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#多核MMD（Multi-kernel-MMD）"><span class="toc-number">1.3.1.</span> <span class="toc-text">多核MMD（Multi-kernel MMD）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多层适配"><span class="toc-number">1.3.2.</span> <span class="toc-text">多层适配</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#优化目标"><span class="toc-number">1.3.3.</span> <span class="toc-text">优化目标</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#网络参数学习"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">网络参数学习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Beyond-Sharing-Weights-for-Deep-Domain-Adaptation"><span class="toc-number">1.4.</span> <span class="toc-text">Beyond Sharing Weights for Deep Domain Adaptation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deep-CORAL-Correlation-Alignment-for-Deep-Domain-Adaptation"><span class="toc-number">1.5.</span> <span class="toc-text">Deep CORAL: Correlation Alignment for Deep Domain Adaptation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deep-Domain-Adaptation-by-Geodesic-Distance-Minimization"><span class="toc-number">1.6.</span> <span class="toc-text">Deep Domain Adaptation by Geodesic Distance Minimization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#多领域适应"><span class="toc-number">1.7.</span> <span class="toc-text">多领域适应</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">NYY</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/niuyuanyuanna" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">37</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">8</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://www.ouyangsong.com" target="_blank">欧阳松的博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://github.com/niuyuanyuanna/BlogImages/raw/master/background/deep_learning.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">NYY's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">迁移学习--样本自适应</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-08</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Deep-Learning/">Deep Learning</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/2018/11/08/deep_learning/transfer-learning/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2018/11/08/deep_learning/transfer-learning/"></span></a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2,393</span><span class="post-meta__separator">|</span><span>Reading time: 9 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="迁移学习——样本自适应"><a href="#迁移学习——样本自适应" class="headerlink" title="迁移学习——样本自适应"></a>迁移学习——样本自适应</h1><h2 id="DaNN"><a href="#DaNN" class="headerlink" title="DaNN"></a>DaNN</h2><p>2014年提出(Domain adaptive Neural Networks for Object Recognition)</p>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>两层神经元组成：特征层和分类器层 </p>
<h3 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h3><p>在特征层加入MMD适配层，用以计算源域和目标域的距离，网络的优化目标为：</p>
<ul>
<li>在有label的源域数据上的分类误差$\ell_{C}$</li>
<li>对两个domain数据的判别误差$\ell_{D}$</li>
</ul>
<p>因此，优化目标为：$\ell=\ell_{C} + \lambda \ell_{D}$，其中$\lambda$为网络适配权重参数。</p>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>由于网络太浅，表征能力有限，故无法很有效地解决domain adaptation问题（通俗点说就是精度不高）。<br>因此，后续的研究者大多数都基于其思想进行扩充，如将浅层网络改为更深层的AlexNet、ResNet、VGG等；<br>如将MMD换为多核的MMD等。</p>
<h3 id="MMD"><a href="#MMD" class="headerlink" title="MMD"></a>MMD</h3><p>最大均值差异（Maximum mean discrepancy），度量在再生希尔伯特空间中两个分布的距离，是一种核学习方法。</p>
<p>两个随机变量的距离为：<br>$$<br>\begin{equation}<br>MMD[\mathfrak{F},X,Y]=[\frac{1}{m^{2}}\sum_{i,j=1}^{m}k(x_{i},y_{j}) - \frac{2}{mn}\sum_{i,j=1}^{m,n}k(x_{i},y_{j}) + \frac{1}{n^{2}}\sum_{i,j=1}^{n}k(x_{i},y_{j})]^{\frac{1}{2}}<br>\end{equation}<br>$$<br>其中</p>
<ul>
<li>$k()$是映射关系，类似于SVM中的核函数，把原变量映射到高维空间;</li>
<li>X，Y为两种分布的样本，</li>
<li>$\mathfrak{F}$表示映射函数集 </li>
</ul>
<p>基于两个分布的样本，通过寻找在样本空间上的映射函数k，求不同分布的样本在k上的函数值的均值，通过把两个均值作差可以得到两个分布对应于k的mean discrepancy。寻找一个k使得这个mean discrepancy有最大值，就得到了MMD。<br>最后取MMD作为检验统计量（test statistic），从而判断两个分布是否相同。如果这个值足够小，就认为两个分布相同，否则就认为它们不相同。更加简单的理解就是：求两堆数据在高维空间中的均值的距离。<br>近年来，MMD越来越多地应用在迁移学习中。在迁移学习环境下训练集和测试集分别取样自分布p和q，两类样本集不同但相关。可以利用深度神经网络的特征变换能力，来做特征空间的变换，直到变换后的特征分布相匹配，这个过程可以是source domain一直变换直到匹配target domain。匹配的度量方式就是MMD。</p>
<h2 id="DDC-Deep-Domain-Confusion"><a href="#DDC-Deep-Domain-Confusion" class="headerlink" title="DDC(Deep Domain Confusion)"></a>DDC(Deep Domain Confusion)</h2><p>Deep Domain Confusion: Maximizing for Domain Invariance发表于2014年。DDC针对预训练的AlexNet（8层）网络，在第7层（也就是feature层，softmax的上一层）加入了MMD距离来减小source和target之间的差异。</p>
<div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/deepLearning/66547922.jpg" alt="DDC structure" title="DDC structure" width="50%/"><br></div>

<p>DDC在原有的AlexNet结构中，对网络的fc7添加一层适配器(Adaption layer)，单独考察网络对源域和目标域的判断能力。若判别能力很差就认为网络学到的特征不足以将两个领域数据区分开，有助于学习到对领域不敏感的特征表示。DDC是深度网络应用于迁移学习领域的经典作品。</p>
<h2 id="DAN-Deep-Adaptation-Networks"><a href="#DAN-Deep-Adaptation-Networks" class="headerlink" title="DAN(Deep Adaptation Networks)"></a>DAN(Deep Adaptation Networks)</h2><p>DAN（2015）是在DDC的基础上发展而来，解决了DDC的两个问题：</p>
<ul>
<li>DDC只适配一层网络，效果不够好，因为不同层都是可以迁移的，因此DAN可以多适配几层</li>
<li>DDC只用了单一核的MMD，这个核可能不是最优的核，因此采用多核的MMD，即MK-MMD。</li>
</ul>
<p>DAN的网络结构为：</p>
<div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/deepLearning/12001851.jpg" alt="DAN structure" title="DAN structure" width="80%/"><br></div>


<h3 id="多核MMD（Multi-kernel-MMD）"><a href="#多核MMD（Multi-kernel-MMD）" class="headerlink" title="多核MMD（Multi-kernel MMD）"></a>多核MMD（Multi-kernel MMD）</h3><p>MMD主要思想是把source和target用相同的映射方法映射到同一个再生核希尔伯特空间（RKHS）中，然后求映射后两部分的均值差异，作为两部分数据的差异。在MMD中这个核函数是固定的，在实现时可以选择是高斯核还是线性核使用单一的核函数，无法确定哪个核函数好。因此使用多个核构造总的核。</p>
<p>对于两个概率分布$p$，$q$，它们之间的MK-MMD为：<br>$$<br>d^2_k(p,q) \triangleq ||E_p[\phi(\mathbf{x}_s)]-E_q[\phi(\mathbf{x}<em>t)]||^2</em>{\mathcal{H}}<br>$$<br>多个核一起定义的kernel为：<br>$$<br>\mathcal{K} \triangleq \left{k= \sum_{u=1}^{m}\beta_u k_u : \beta_u \ge 0, \forall u \right}<br>$$</p>
<p>用$m$个不同的kernel加权，权重为$\beta_u$，得到的$\mathcal{K}$表征能力比单核更强。</p>
<h3 id="多层适配"><a href="#多层适配" class="headerlink" title="多层适配"></a>多层适配</h3><p>DDC方法中，只适配AlexNet的第七层，DAN仍然基于AlexNet网络，适配最后三个全连接层。因为网络的迁移能力在这三层会task-spacific，类似于Inception、ResNet等，在迁移学习时，只学习最后一层全连接层的参数，经过softmax层后得到分类信息。</p>
<h3 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h3><p>DAN方法，基于AlexNet网络，探索source和target之间的适配关系。任何一个方法都有优化的目标。DAN也不例外。它的优化目标由两部分组成：损失函数和分布距离。基本上所有的机器学习方法都会定义一个损失函数，它来度量预测值和真实值的差异。分布距离就是面提到的MK-MMD距离。于是，DAN的优化目标就是：</p>
<p>$$<br>\min_\Theta \frac{1}{n_a} \sum_{i=1}^{n_a} J(\theta(\mathbf{x}^a_i),y^a_i) + \lambda \sum_{l=l_1}^{l_2}d^2_k(\mathcal{D}^l_s,\mathcal{D}^l_t)<br>$$</p>
<ul>
<li>$\Theta$表示所有权重和bias参数，是需要学习得到的目标参数；</li>
<li>$l_1$、$l_2$表示网络适配从第六层到第八层，前面的网络不进行适配；</li>
<li>$\mathbf{x}_a$，$n_a$表示source和target中所有的label数据集合；</li>
<li>$\lambda$为惩罚系数；</li>
<li>$J(.)$定义一个损失函数，一般使用cross-entropy。</li>
</ul>
<p>损失函数的前面部分是网络参数$\Theta$，后面部分为MMD的距离参数$\beta$</p>
<h4 id="网络参数学习"><a href="#网络参数学习" class="headerlink" title="网络参数学习"></a>网络参数学习</h4><p>对$\Theta$的学习依赖于MK-MMD距离的计算。通过kernel trick（类比于以前的MMD距离）总是可以把MK-MMD展开成一堆内积的形式。然而，数据之间两两计算内积是非常复杂的，时间复杂度为$O(n^2)$，这个在深度学习中的开销非常大。<br>因此，提出对MK-MMD的无偏估计：</p>
<p>$$<br>d^2_k(p,q)=\frac{2}{n_s}\sum_{i=1}^{n_s/2}g_k(\mathbf{z}_i)\<br>\mathbf{z}<em>i \triangleq (\mathbf{x}^s</em>{2i-1},\mathbf{x}^s_{2i},\mathbf{x}^t_{2i-1},\mathbf{x}^t_{2i})<br>$$</p>
<p>将kernel作用到$\mathbf{z}_i$上，变为：<br>$$<br>g_k(\mathbf{z}<em>i) \triangleq k(\mathbf{x}^s</em>{2i-1},\mathbf{x}^s_{2i})+k(\mathbf{x}^t_{2i-1},\mathbf{x}^t_{2i})-k(\mathbf{x}^s_{2i-1},\mathbf{x}^t_{2i})-k(\mathbf{x}^s_{2i},\mathbf{x}^t_{2i-1})<br>$$<br>只计算了连续的一对数据的距离，再乘以2，这样就可以把时间复杂度降低到$O(n)$。在具体进行SGD的时候，需要对所有的参数求导：对$\Theta$求导。在实际用multiple-kernel的时候，作者用多个高斯核。</p>
<h2 id="Beyond-Sharing-Weights-for-Deep-Domain-Adaptation"><a href="#Beyond-Sharing-Weights-for-Deep-Domain-Adaptation" class="headerlink" title="Beyond Sharing Weights for Deep Domain Adaptation"></a>Beyond Sharing Weights for Deep Domain Adaptation</h2><p>发表于2016年，提出了在适配层中target和source不共享参数的思想。与以往的Domain adaptation在Deep Learing中的运用不同，这篇论文提出，在source domain和target domain之间使用不同的参数而非共享参数（不是所有的layers都不共享参数，有些层还是共享了）。他们提出在试验中这种网络的表现会好于那些使用共享参数的网络。</p>
<p>其网络结构为双流结构：</p>
<div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/deepLearning/65970249.jpg" alt="BSW for DDA" title="BSW for DDA" width="70%/"><br></div>

<p>该结构引入loss防止两个分支对应的权重差异过大。</p>
<h2 id="Deep-CORAL-Correlation-Alignment-for-Deep-Domain-Adaptation"><a href="#Deep-CORAL-Correlation-Alignment-for-Deep-Domain-Adaptation" class="headerlink" title="Deep CORAL: Correlation Alignment for Deep Domain Adaptation"></a>Deep CORAL: Correlation Alignment for Deep Domain Adaptation</h2><p>这篇文章发表于2016年，主要提出一个CORAL loss，通过对source domain和target domain进行线性变换将各自的二阶统计量对齐。<br>$$<br>L_{CORAL} = \frac{1}{4d^2} ||C_S - C_T||^2 \<br>C_S = \frac{1}{n_S - 1}(D_S^TD_S - \frac{1}{n_S} (1^TD_S)^T (1^TD_S)) \<br>C_T = \frac{1}{n_T - 1}(D_T^TD_T - \frac{1}{n_T} (1^TD_T)^T (1^TD_T))<br>$$<br>其中</p>
<ul>
<li><p>$n_S$，$n_T$为source domain和target domain的batch size；</p>
</li>
<li><p>$d$为特征的维度；</p>
</li>
<li>$1^T$为一个全1的向量</li>
</ul>
<p>其网络结构为：</p>
<div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/deepLearning/32435164.jpg" alt="Deep CORAL" title="Deep CORAL" width="80%/"><br></div>


<h2 id="Deep-Domain-Adaptation-by-Geodesic-Distance-Minimization"><a href="#Deep-Domain-Adaptation-by-Geodesic-Distance-Minimization" class="headerlink" title="Deep Domain Adaptation by Geodesic Distance Minimization"></a>Deep Domain Adaptation by Geodesic Distance Minimization</h2><p>文章发表于2017年，在CORAL loss的基础上将其改进为Log-CORAL loss。表示两个协方差矩阵的log之间的欧氏距离。公式为：<br>$$<br>L_{LogCORAL} = \frac{1}{4d^2} ||log(C_S) - log(C_T)||^2 \<br>$$<br>其网络结构为：</p>
<div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/deepLearning/24542894.jpg" alt="Log-CORAL loss" title="Log-CORAL loss" width="80%/"><br></div>

<p>特征迁移目前都是小网络进行尝试，常用的特征分布的度量标准是MMD和CORAL等，但其在网络中的位置，以及损失核K的选取都很关键，并且发现多应用于FC层，哪些层应该或不应该共享其权重的最佳选择取决于实际的应用。 </p>
<p>FC可在模型表示能力迁移过程中充当“防火墙”的作用。具体来讲，假设在ImageNet上预训练得到的模型为M ，则ImageNet可视为源域（迁移学习中的source domain）。微调（fine tuning）是深度学习领域最常用的迁移学习技术。针对微调，若目标域（target domain）中的图像与源域中图像差异巨大（如相比ImageNet，目标域图像不是物体为中心的图像，而是风景照），不含FC的网络微调后的结果要差于含FC的网络。因此FC可视作模型表示能力的“防火墙”，特别是在源域与目标域差异较大的情况下，FC可保持较大的模型capacity从而保证模型表示能力的迁移。（冗余的参数并不一无是处。）</p>
<h2 id="多领域适应"><a href="#多领域适应" class="headerlink" title="多领域适应"></a>多领域适应</h2><p>《Deep Cocktail Network: Multi-source Unsupervised Domain Adaptation with Category Shift》发表于2018年。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">NYY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2018/11/08/deep_learning/transfer-learning/">http://yoursite.com/2018/11/08/deep_learning/transfer-learning/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/CV/">CV</a><a class="post-meta__tags" href="/tags/transfer-learning/">transfer learning</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/11/08/deep_learning/data-normalization/"><i class="fa fa-chevron-left">  </i><span>深度学习中数据预处理方法</span></a></div><div class="next-post pull-right"><a href="/2018/11/08/offer_problem/leedcode4/"><span>leedcode4</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://yoursite.com/2018/11/08/deep_learning/transfer-learning/';
  this.page.identifier = '2018/11/08/deep_learning/transfer-learning/';
  this.page.title = '迁移学习--样本自适应';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'niuyuanyuan' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-src" src="https://niuyuanyuan.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 By NYY</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>