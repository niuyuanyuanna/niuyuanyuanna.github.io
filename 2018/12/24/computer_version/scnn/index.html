<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="scnn论文笔记"><meta name="keywords" content="计算机视觉,Action Recognition,论文笔记"><meta name="author" content="NYY,undefined"><meta name="copyright" content="NYY"><title>scnn论文笔记 | NYY's blog</title><link rel="shortcut icon" href="/img/my_icon.jpg"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Temporal-action-localization-in-untrimmed-videos-via-multi-stage-cnns论文笔记"><span class="toc-number">1.</span> <span class="toc-text">Temporal action localization in untrimmed videos via multi-stage cnns论文笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#网络模型"><span class="toc-number">1.1.</span> <span class="toc-text">网络模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#多尺度视频片段生成"><span class="toc-number">1.1.1.</span> <span class="toc-text">多尺度视频片段生成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C3D网络"><span class="toc-number">1.1.2.</span> <span class="toc-text">C3D网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Proposal-Network"><span class="toc-number">1.1.3.</span> <span class="toc-text">Proposal Network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classification-Network"><span class="toc-number">1.1.4.</span> <span class="toc-text">Classification Network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Localization-Network"><span class="toc-number">1.1.5.</span> <span class="toc-text">Localization Network</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">NYY</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/niuyuanyuanna" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">41</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">32</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">8</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://www.ouyangsong.com" target="_blank">欧阳松的博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://github.com/niuyuanyuanna/BlogImages/raw/master/background/computer_version.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">NYY's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">scnn论文笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-24</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Computer-Version/">Computer Version</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/2018/12/24/computer_version/scnn/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2018/12/24/computer_version/scnn/"></span></a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1,287</span><span class="post-meta__separator">|</span><span>Reading time: 4 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Temporal-action-localization-in-untrimmed-videos-via-multi-stage-cnns论文笔记"><a href="#Temporal-action-localization-in-untrimmed-videos-via-multi-stage-cnns论文笔记" class="headerlink" title="Temporal action localization in untrimmed videos via multi-stage cnns论文笔记"></a>Temporal action localization in untrimmed videos via multi-stage cnns论文笔记</h1><p><a href="http://dvmmweb.cs.columbia.edu/files/dvmm_scnn_paper.pdf" target="_blank" rel="noopener">Temporal action localization in untrimmed videos via multi-stage cnns</a>是Zheng Shou发表在在CVPR2016上的论文，主要解决视频识别中的两个问题：</p>
<ul>
<li>Action Recognition：目的为判断一个已经分割好的短视频片段的类别。特点是简化了问题，一般使用的数据库都先将动作分割好了，一个视频片断中包含一段明确的动作，时间较短（几秒钟）且有唯一确定的label。所以也可以看作是输入为视频，输出为动作标签的多分类问题。常用数据库包括UCF101，HMDB51等。</li>
<li>Temporal Action Location：不仅要知道一个动作在视频中是否发生，还需要知道动作发生在视频的哪段时间（包括开始和结束时间）。特点是需要处理较长的，未分割的视频。且视频通常有较多干扰，目标动作一般只占视频的一小部分。常用数据库包括THUMOS2014/2015, ActivityNet等。</li>
</ul>
<p>这篇文章主要解决Temporal Action Localization的问题。SCNN指segment based CNN,即基于视频片段的CNN网络。</p>
<p>SCNN主要包括三个部分：</p>
<ol>
<li>多尺度视频片段生成</li>
<li>动作分类网络</li>
<li>分类网络微调</li>
</ol>
<p>阅读论文之前可以阅读iDT论文</p>
<h2 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h2><div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/SCNN.png" width="100%"><br></div>

<p>使用的每一段视频标注为$\Psi = {  (\psi_m, \psi_m^{‘} , k_m ) }_{m = 1}^M $，其中$\psi_m$为动作起始时间，$\psi^{‘}_m$为动作终止时间，$k_m$为该动作的id。$k_m \in { 1,2,…,K }$，$K$为动作的种类。在训练过程中共有两种视频，一种是修剪过的视频$X \in \mathbb{T}$，对于修剪过的视频$\psi_m = 1$， $\psi_m^{‘} = T$  ，$M=1$；另一种是未修剪的视频$X \in \mathbb{U}$</p>
<h3 id="多尺度视频片段生成"><a href="#多尺度视频片段生成" class="headerlink" title="多尺度视频片段生成"></a>多尺度视频片段生成</h3><p>第一步为生成候选的视频片段，作为下一阶段网络的输入。</p>
<p>首先将每一帧视频resize到$171*128$像素大小，对于未修剪过的视频，使用滑窗的方法产生视频片段，包括多个长度帧的片段：16,32,64,128,256,512。视频帧的重复度为75%。对于每一个未修剪视频，得到一系列视频片段：$\Phi={ (s_h, \phi_h, \phi_h^{‘} ) }^H_{h = 1}$，其中$H$是滑窗的个数，$\phi_h$和$\phi_h^{‘}$是第$h$个片段$s_h$的起始时间和终止时间。</p>
<p>在得到视频片段后，对其进行平均采样16帧视频，从而使得输出的segment的长度均为16。</p>
<h3 id="C3D网络"><a href="#C3D网络" class="headerlink" title="C3D网络"></a>C3D网络</h3><p>C3D网络来自于<a href="https://arxiv.org/abs/1412.0767" target="_blank" rel="noopener">learning Spatiotemporal feature with 3DConvolutional Networks</a>这篇文章，前面的博客介绍过C3D网络及特征。</p>
<h3 id="Proposal-Network"><a href="#Proposal-Network" class="headerlink" title="Proposal Network"></a>Proposal Network</h3><p>在生成训练数据时，同时还记录和segment和ground truth instance之间的最大重叠度（IoU)。对于proposal网络来说，将最大IoU大于0.7的标记为true，最大IoU小于0.3的标记为背景。以及类别（即如果存在多个重叠的ground truth,取重叠度最大的那个）。</p>
<p>将得到的所有片段输入到C3D网络中，经过fc8后分为两类，即判断是否为背景，训练时将IoU大于0.7的作为正样本（动作），小于0.3的作为负样本（背景），对负样本进行采样使得正负样本比例均衡。采用softmax loss进行训练。proposal network的主要作用是去除一些背景片段。</p>
<h3 id="Classification-Network"><a href="#Classification-Network" class="headerlink" title="Classification Network"></a>Classification Network</h3><p>经过Proposal Network后，背景被去除，对剩下的数据进行$K$个类别的动作分类。</p>
<p>和Proposal Network类似，经过fc8后输出$K+1$类，其中一类是背景<br>输出为K+1个类别（包括背景类）的分数, 这个网络被用来初始化localization network, 仅在训练阶段使用，在测试阶段不使用。训练时同样将IoU大于0.7的作为正样本（K类动作），小于0.3的作为背景类，对背景类动作进行采样使得背景类动作的数量和K类动作数量的平均值相近。训练时同样采用softmax loss。</p>
<h3 id="Localization-Network"><a href="#Localization-Network" class="headerlink" title="Localization Network"></a>Localization Network</h3><p>在估计动作的起始位置时，需要抑制与groundtruth重叠部分很小的片段，增强重叠部分大的片段，如下面图中的B片段才是真正需要保留的，AC应该去除。</p>
<p><div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/LocalizationNetwork.png" width="60%"><br></div><br>为了达到这样的目标，在loss function增加了overlap的损失。在一个minibatch中，有N个训练样本${ (s_n, k_n, v_n) }_{n = 1}^N$，对于第n个片段，fc8的输出向量为$O_n$，经过softmax后得到的预测结果为$P_n$，计算方式为：<br>$$<br>P_n^{(i)} = \frac{e^{O_n^{(i)}}}{\sum_{j=1}^N e^{O_n^{(i)}}}<br>$$<br>新的loss function为：<br>$$<br>L = L_{softmax} + \lambda \cdot L_{overlap}<br>$$<br>其中softmax损失函数为：<br>$$<br>L_{softmax} = \frac{1}{N} \sum_n(-log(P_n^{k_n}))<br>$$<br>是动作分类的损失函数，overlap的损失函数为：<br>$$<br>L_{overlap} = \frac{1}{N}\sum_n(\frac{1}{2} \cdot (\frac{(P_n^{k_n})^2}{(v_n)^\alpha} -1) \cdot [k_n &gt; 0])<br>$$<br>这里的$k_n &gt;0$表示$[k_n &gt;0] = 1$时，该片段是动作，$[k_n &gt;0] = 0$时，该片段是背景。$L_{overlap}$主要是为了增强与GT有很大重合度的片段的得分，抑制重合度较低的片段。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">NYY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2018/12/24/computer_version/scnn/">http://yoursite.com/2018/12/24/computer_version/scnn/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/计算机视觉/">计算机视觉</a><a class="post-meta__tags" href="/tags/Action-Recognition/">Action Recognition</a><a class="post-meta__tags" href="/tags/论文笔记/">论文笔记</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/12/27/computer_version/C3D/"><i class="fa fa-chevron-left">  </i><span></span></a></div><div class="next-post pull-right"><a href="/2018/12/19/computer_version/idt/"><span>IDT算法论文笔记</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://yoursite.com/2018/12/24/computer_version/scnn/';
  this.page.identifier = '2018/12/24/computer_version/scnn/';
  this.page.title = 'scnn论文笔记';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'niuyuanyuan' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-src" src="https://niuyuanyuan.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 By NYY</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>