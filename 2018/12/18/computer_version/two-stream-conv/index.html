<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Two-Stream Convolutional Networks for Action Recognition in Videos论文笔记"><meta name="keywords" content="计算机视觉,Action Recognition,论文笔记"><meta name="author" content="NYY,undefined"><meta name="copyright" content="NYY"><title>Two-Stream Convolutional Networks for Action Recognition in Videos论文笔记 | NYY's blog</title><link rel="shortcut icon" href="/img/my_icon.jpg"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Two-Stream-Convolutional-Networks-for-Action-Recognition-in-Videos论文笔记"><span class="toc-number">1.</span> <span class="toc-text">Two-Stream Convolutional Networks for Action Recognition in Videos论文笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#主要贡献"><span class="toc-number">1.1.</span> <span class="toc-text">主要贡献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#相关工作"><span class="toc-number">1.2.</span> <span class="toc-text">相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基于时空域特征的浅层高维编码"><span class="toc-number">1.2.1.</span> <span class="toc-text">基于时空域特征的浅层高维编码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HOG（Histogram-of-Oriented-Gradient）"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">HOG（Histogram of Oriented Gradient）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#步骤"><span class="toc-number">1.2.1.1.1.</span> <span class="toc-text">步骤</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#优点"><span class="toc-number">1.2.1.1.2.</span> <span class="toc-text">优点</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HOF（Histogram-of-Flow）"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">HOF（Histogram of Flow）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Optical-Flow"><span class="toc-number">1.2.1.2.1.</span> <span class="toc-text">Optical Flow</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#步骤-1"><span class="toc-number">1.2.1.2.2.</span> <span class="toc-text">步骤</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#补充"><span class="toc-number">1.2.1.2.3.</span> <span class="toc-text">补充</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于密集轨迹点"><span class="toc-number">1.2.2.</span> <span class="toc-text">基于密集轨迹点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于神经网络"><span class="toc-number">1.2.3.</span> <span class="toc-text">基于神经网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Two-Stream结构"><span class="toc-number">1.3.</span> <span class="toc-text">Two-Stream结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#光流分支"><span class="toc-number">1.3.1.</span> <span class="toc-text">光流分支</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多任务学习"><span class="toc-number">1.3.2.</span> <span class="toc-text">多任务学习</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">NYY</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/niuyuanyuanna" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">38</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">32</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">8</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://www.ouyangsong.com" target="_blank">欧阳松的博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://github.com/niuyuanyuanna/BlogImages/raw/master/background/computer_version.jpeg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">NYY's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">Two-Stream Convolutional Networks for Action Recognition in Videos论文笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-18</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Computer-Version/">Computer Version</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/2018/12/18/computer_version/two-stream-conv/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2018/12/18/computer_version/two-stream-conv/"></span></a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2,786</span><span class="post-meta__separator">|</span><span>Reading time: 10 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Two-Stream-Convolutional-Networks-for-Action-Recognition-in-Videos论文笔记"><a href="#Two-Stream-Convolutional-Networks-for-Action-Recognition-in-Videos论文笔记" class="headerlink" title="Two-Stream Convolutional Networks for Action Recognition in Videos论文笔记"></a>Two-Stream Convolutional Networks for Action Recognition in Videos论文笔记</h1><p>论文原文地址：<a href="https://arxiv.org/abs/1406.2199" target="_blank" rel="noopener">Two-Stream Convolutional Networks for Action Recognition in Videos</a></p>
<p>这篇文章是NIPS 2014年提出一个two stream网络来做video action的分类，比较经典。two stream表示两个并行的网络：spatial stream convnet 和 temporal stream convnet. 这两个并行网络的作用是：空间域上从静态图像帧中识别动作；时域上使用密集光流特征识别动作。最后进行信息融合。</p>
<h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><ol>
<li>提出了结合空域和时域网络的two_stream卷积网络结构。</li>
<li>验证了即使在较小规模的训练数据集上，在多帧稠密光流上训练的卷积神经网络可以获得非常好的性能。</li>
<li>展示了多任务学习，应用于不同的运动分类数据集，可以同时提升数据集的规模和检测性能。</li>
</ol>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="基于时空域特征的浅层高维编码"><a href="#基于时空域特征的浅层高维编码" class="headerlink" title="基于时空域特征的浅层高维编码"></a>基于时空域特征的浅层高维编码</h3><p>该方法是神经网络还未出现的传统视频处理方法。</p>
<h4 id="HOG（Histogram-of-Oriented-Gradient）"><a href="#HOG（Histogram-of-Oriented-Gradient）" class="headerlink" title="HOG（Histogram of Oriented Gradient）"></a>HOG（Histogram of Oriented Gradient）</h4><p>为梯度方向直方图，通过计算和统计图像局部区域的梯度直方图构成特征。因为其是在静态图像中提取的，所以为空间特征。<br>HOG用于目标检测。一种解决人体目标检测的图像描述子，是一种用于表征图像局部梯度方向和梯度强度分布特性的描述符</p>
<h5 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h5><ol>
<li>灰度化，标准化gamma空间和颜色空间；</li>
</ol>
<p>gamma压缩公式：</p>
<p>$$<br>I_{(x,y)} = I_{(x,y)}^{gamma}<br>$$</p>
<ol start="2">
<li>计算图像在x方向和y方向的梯度</li>
</ol>
<p>常用的计算梯度的方法是使用Sobel算子和图像卷积，得到x方向和y方向的梯度：<br>$$<br>G_x = \begin{bmatrix}<br>-1 &amp; 0 &amp; 1<br>\end{bmatrix}\ast A \<br>G_y = \begin{bmatrix}<br>-1 \<br>0 \<br>1<br>\end{bmatrix}\ast A<br>$$<br>得到梯度特征的幅值和夹角<br>$$<br>G = \sqrt{G_x^2 + G_y^2} \<br>\theta = arctan \frac{G_y}{G_x}<br>$$</p>
<ol start="3">
<li>为每个cell构建梯度直方图<br>假设每个cell像素为$8 \ast 8$，切块后根据下面的星状图将对应夹角的幅值增加到对应的块中，形成一个$1 \ast 9$的特征向量。</li>
</ol>
<div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/HOF.png" width="75%"><br></div>

<ol start="4">
<li>将多个cell拼接为1个block，归一化得到block的特征向量</li>
</ol>
<ul>
<li>对每个cell特征向量归一化</li>
<li>拼接$n \ast n$个cell为一个block，特征向量此时变为$1 \ast n^2$</li>
<li>归一化1个block的特征向量，滑窗式移动到下一个block</li>
</ul>
<ol start="5">
<li>将所有block的特征vector拼接为特征matrix</li>
</ol>
<h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><ol>
<li>对图像几何的和光学的形变都能保持很好的不变性</li>
<li>在粗的空域抽样、精细的方向抽样以及较强的局部光学归一化等条件下，只要行人大体上能够保持直立的姿 势，可以容许行人有一些细微的肢体动作，这些细微的动作可以被忽略而不影响检测效果。</li>
</ol>
<h4 id="HOF（Histogram-of-Flow）"><a href="#HOF（Histogram-of-Flow）" class="headerlink" title="HOF（Histogram of Flow）"></a>HOF（Histogram of Flow）</h4><p>HOF是基于光流法的直方图，因此需要了解光流法的计算。</p>
<h5 id="Optical-Flow"><a href="#Optical-Flow" class="headerlink" title="Optical Flow"></a>Optical Flow</h5><p>光流是由于场景中前景目标本身的移动、相机的运动，或者两者的共同运动所产生的。</p>
<ul>
<li>其计算方法可以分为三类：</li>
</ul>
<ol>
<li>基于区域或者基于特征的匹配方法</li>
<li>基于频域的方法</li>
<li>基于梯度的方法</li>
</ol>
<p>简单来说，光流是空间运动物体在观测成像平面上的像素运动的“瞬时速度”。光流的研究是利用图像序列中的像素强度数据的时域变化和相关性来确定各自像素位置的“运动”。研究光流场的目的就是为了从图片序列中近似得到不能直接得到的运动场。</p>
<ul>
<li>前提假设：</li>
</ul>
<ol>
<li>相邻帧之间的亮度恒定</li>
<li>相邻视频帧的取帧时间连续，或者，相邻帧之间物体的运动比较“微小”</li>
<li>保持空间一致性；即，同一子图像的像素点具有相同的运动</li>
</ol>
<p>运动场，其实就是物体在三维真实世界中的运动；光流场，是运动场在二维图像平面上的投影。</p>
<div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/OF.png" width="75%"><br></div>

<ul>
<li>目的：</li>
</ul>
<p>对于图片中的每个像素点找到其速度向量$\vec{u} = (u, v)$。</p>
<ul>
<li>步骤：</li>
</ul>
<ol>
<li>由于亮度恒定和微小运动，可以假设像素点灰度$I_{(x,y,t)} = I_{(x+dx, y+dy, t+dt)}$</li>
<li>对上式进行Taylor一阶展开：</li>
</ol>
<p>$$<br>I_{(x+dx, y+dy, t+dt)} = I_{(x,y,t)} + \frac{\partial I}{\partial x} dx + \frac{\partial I}{\partial y} dy + \frac{\partial I}{\partial t} dt<br>$$</p>
<p>得到：<br>$$<br>I_xdx + I_ydy + I_tdt = 0<br>$$<br>令$u = \frac{dx}{dt}$, $v = \frac{dy}{dt}$，则得到：<br>$$<br>I_xu + I_yv = -I_t<br>$$<br>即：<br>$$<br>\begin{bmatrix}<br> I_x &amp; I_y<br>\end{bmatrix} \ast<br>\begin{bmatrix}<br> u \<br> v<br>\end{bmatrix}<br>=-I_t<br>$$<br>在一个小的邻域内，亮度恒定，则在这个邻域内的像素点满足：<br>$$<br>\begin{bmatrix}<br> I_{x_1} &amp; I_{y_1} \<br> I_{x_2} &amp; I_{y_2} \<br> \vdots  &amp;  \vdots<br>\end{bmatrix} \ast<br>\begin{bmatrix}<br> u \<br> v<br>\end{bmatrix} = -<br>\begin{bmatrix}<br> I_{t_1}  \<br> I_{t_2} \<br> \vdots<br>\end{bmatrix}<br>$$<br>即需要满足$A \vec{u} = b$，因此光流法主要是为了使得$\left | A \vec{u} -b \right |^2$有最小值。</p>
<ol start="3">
<li>若$A \vec{u} = b$，则可以使用矩阵计算出$\vec{u} = (A^TA)^{-1} A^Tb$</li>
</ol>
<ul>
<li>光流法用于目标跟踪的原理：</li>
</ul>
<ol>
<li>对一个连续的视频帧序列进行处理</li>
<li>针对每一个视频序列，利用一定的目标检测方法，检测可能出现的前景目标</li>
<li>如果某一帧出现了前景目标，找到其具有代表性的关键特征点（可以随机产生，也可以利用角点来做特征点）</li>
<li>对之后的任意两个相邻视频帧而言，寻找上一帧中出现的关键特征点在当前帧中的最佳位置，从而得到前景目标在当前帧中的位置坐标</li>
<li>如此迭代进行，便可实现目标的跟踪</li>
</ol>
<h5 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h5><ol>
<li>计算每帧图像对应的光流场$\vec{u} =\begin{bmatrix} u \ v \end{bmatrix} $</li>
<li>计算光流矢量与横轴夹角及幅值：</li>
</ol>
<p>$$<br>\theta = arctan \frac{v}{u} \<br>U = \sqrt{u^2 + v^2}<br>$$</p>
<ol start="3">
<li>使用同HOG相同的方法将夹角分为几个区域，绘制直方图，得到特征向量</li>
<li>归一化特征向量</li>
</ol>
<h5 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h5><ol>
<li>以横轴为基准计算夹角能够使HOF特征对运动方向（向左和向右）不敏感</li>
<li>通过归一化直方图实现HOF特征的尺度不变性</li>
<li>HOF直方图通过光流幅值加权得到，因此小的背景噪声对直方图的影响微乎其微</li>
<li>通常直方图bin取30以上识别效果较好</li>
</ol>
<p>HOF提取后的特征被编码为BOF（特征词袋）表示，并使用SVM进行线性分类</p>
<h3 id="基于密集轨迹点"><a href="#基于密集轨迹点" class="headerlink" title="基于密集轨迹点"></a>基于密集轨迹点</h3><p>dense point trajectories ，这部分为主要为IDT，由调整局部描述符支持区域组成，可以跟随轨迹，通过光流计算。论文待整理。</p>
<h3 id="基于神经网络"><a href="#基于神经网络" class="headerlink" title="基于神经网络"></a>基于神经网络</h3><p>这些工作的大多数，网络的输入为堆叠的连续的视频帧，网络的输入为堆叠的连续的视频帧，所以模型被希望能够在第一层学习到时空域基于运动的特征。</p>
<h2 id="Two-Stream结构"><a href="#Two-Stream结构" class="headerlink" title="Two-Stream结构"></a>Two-Stream结构</h2><p>网络结构由两部分组成，一个是空域上的，以单个的视频帧表象的形式存在，携带视频中的场景和目标信息；一个是时域上的，以视频帧间的运行形式存在，传递观察者(相机)和目标的移动。<br>其结构为：</p>
<div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/TwoStream.png" width="90%"><br></div>


<p>class score fusion考虑两种融合方案：一种是训练多分类的线性SVM，另一种为基于堆叠的L2-规范化的softmax分数</p>
<h3 id="光流分支"><a href="#光流分支" class="headerlink" title="光流分支"></a>光流分支</h3><p>输入为<strong>一些连续视频帧的堆叠光流位移场</strong></p>
<div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/OpticalFlow.png" width="90%"><br></div><br>1. (a)表示前一帧图像，(b)表示后一帧图像<br>2. (c)表示根据两帧图像计算出的Optical Flow<br>3. (d)(e)分别表示displacement vector field的水平和竖直两部分<br><br>#### 输入<br><br>在第$\tau$帧的时候输入即为$I_\tau$，论文中介绍了两种获取$I_\tau$的方法，一种是Optical Flow Stacking（光流栈），另一种是Trajectory Stacking（轨迹叠加）<br><br>- Optical Flow Stacking<br><br>$I_\tau(u, v, c)$表示$(u,v)$这个位置的像素点的displacement vector（位移矢量），$c$的取值范围为1~2L，其中L表示vedio的帧数，因为有横纵两个方向的vector，因此范围为2L。水平方向和竖直方向的$I_\tau(u, v, c)$计算公式为：<br><br>$$<br>I_\tau(u, v, 2k-1) = d_{\tau + k -1}^{x}(u,v) \<br>I_\tau(u, v, 2k) =  d_{\tau + k -1}^{y}(u,v) \<br>u = [1;w] \quad v=[1;h] \quad k=[1;L]<br>$$<br><br>这种方法是光流的简单叠加。简单的来说就是计算每两帧之间的光流，然后简单的stacking。<br><br><div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/OpticalFlowStacking.png" width="50%"><br></div>

<p>上面的图表示为</p>
<ul>
<li>Trajectory Stacking</li>
</ul>
<p>轨迹叠加就是假设第一帧的某个像素点，我们可以通过光流来追踪它在视频中的轨迹。而简单的光流场叠加并没有追踪，每个都是计算的某帧$\tau + 1$中某个像素点P相对于$\tau$帧中对应像素点q的位移，光流场叠加最终得到的是每个像素点的两帧之间的光流图。<br>$$<br>I_\tau(u, v, 2k-1) = d_{\tau + k -1}^{x}(P_k) \<br>I_\tau(u, v, 2k) =  d_{\tau + k -1}^{y}(P_k) \<br>u = [1;w] \quad v=[1;h] \quad k=[1;L]<br>$$</p>
<p>其中$P_k$是轨迹中的第k个点，其计算方式为：<br>$$<br>P_1 = (u,v) \<br>P_k = P_{k-1} + d_{\tau + k - 2}(p_{k-1})<br>$$</p>
<p><div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/TrajectoryStacking.png" width="60%"><br></div><br>此时$I_\tau(u, v, c)$存储的值为该像素点的运动轨迹。</p>
<ul>
<li>输入帧选择</li>
</ul>
<p>上述两个方法其实考虑的都是前馈光流，我们都是依靠后一帧计算相对于前一帧的光流。当我们考虑T帧时，我们不再一直往后堆L帧，而是计算T帧之前L/2和T帧之后的L/2帧。</p>
<ul>
<li>输入去噪</li>
</ul>
<p>在输入光流前要减去平均光流，排除车相机的运动导致的负面影响。</p>
<ul>
<li>最终输入</li>
</ul>
<p>从$I_\tau$中采样为$224<em>224</em>2L$输入神经网络中。</p>
<h3 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h3><p>spatial stream convnet因为输入是静态的图像，因此其预训练模型容易得到（一般采用在ImageNet数据集上的预训练模型），但是temporal stream convnet的预训练模型就需要在视频数据集上训练得到，但是目前能用的视频数据集规模还比较小（主要指的是UCF-101和HMDB-51这两个数据集，训练集数量分别是9.5K和3.7K个video）。因此作者采用multi-task的方式来解决。首先原来的网络（temporal stream convnet）在全连接层后只有一个softmax层，现在要变成两个softmax层，一个用来计算HDMB-51数据集的分类输出，另一个用来计算UCF-101数据集的分类输出，这就是两个task。这两条支路有各自的loss，最后回传loss的时候采用的是两条支路loss的和。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">NYY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2018/12/18/computer_version/two-stream-conv/">http://yoursite.com/2018/12/18/computer_version/two-stream-conv/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/计算机视觉/">计算机视觉</a><a class="post-meta__tags" href="/tags/Action-Recognition/">Action Recognition</a><a class="post-meta__tags" href="/tags/论文笔记/">论文笔记</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="next-post pull-right"><a href="/2018/12/06/others/picgo/"><span>七牛云图床迁移到github</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://yoursite.com/2018/12/18/computer_version/two-stream-conv/';
  this.page.identifier = '2018/12/18/computer_version/two-stream-conv/';
  this.page.title = 'Two-Stream Convolutional Networks for Action Recognition in Videos论文笔记';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'niuyuanyuan' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-src" src="https://niuyuanyuan.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 By NYY</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>