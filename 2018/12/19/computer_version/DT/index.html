<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="DT算法论文笔记"><meta name="keywords" content="计算机视觉,Action Recognition,论文笔记"><meta name="author" content="NYY,undefined"><meta name="copyright" content="NYY"><title>DT算法论文笔记 | NYY's blog</title><link rel="shortcut icon" href="/img/my_icon.jpg"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Dense-Trajectories-and-Motion-Boundary-Descriptors-for-Action-Recognition论文笔记"><span class="toc-number">1.</span> <span class="toc-text">Dense Trajectories and Motion Boundary Descriptors for Action Recognition论文笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Dense-Trajectories密集轨迹算法"><span class="toc-number">1.1.</span> <span class="toc-text">Dense Trajectories密集轨迹算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dense-Sampling密集采样"><span class="toc-number">1.1.1.</span> <span class="toc-text">Dense Sampling密集采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Trajectory-Shap-Descriptor轨迹形状描述子"><span class="toc-number">1.1.2.</span> <span class="toc-text">Trajectory Shap Descriptor轨迹形状描述子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Motion-and-Structure-Descriptors运动和结构描述子"><span class="toc-number">1.1.3.</span> <span class="toc-text">Motion and Structure Descriptors运动和结构描述子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bag-of-Features特征编码"><span class="toc-number">1.1.4.</span> <span class="toc-text">Bag of Features特征编码</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">NYY</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/niuyuanyuanna" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">42</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">32</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">8</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://www.ouyangsong.com" target="_blank">欧阳松的博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://github.com/niuyuanyuanna/BlogImages/raw/master/background/computer_version.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">NYY's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">DT算法论文笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-19</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Computer-Version/">Computer Version</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/2018/12/19/computer_version/DT/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2018/12/19/computer_version/DT/"></span></a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1,340</span><span class="post-meta__separator">|</span><span>Reading time: 5 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Dense-Trajectories-and-Motion-Boundary-Descriptors-for-Action-Recognition论文笔记"><a href="#Dense-Trajectories-and-Motion-Boundary-Descriptors-for-Action-Recognition论文笔记" class="headerlink" title="Dense Trajectories and Motion Boundary Descriptors for Action Recognition论文笔记"></a>Dense Trajectories and Motion Boundary Descriptors for Action Recognition论文笔记</h1><p>论文原文地址<a href="https://www.researchgate.net/publication/257672334_Dense_Trajectories_and_Motion_Boundary_Descriptors_for_Action_Recognition" target="_blank" rel="noopener">Dense Trajectories and Motion Boundary Descriptors for Action Recognition</a></p>
<p>iDT算法是行为识别领域中非常经典的一种算法，在深度学习应用于该领域前也是效果最好的算法。由INRIA的IEAR实验室于2013年发表于ICCV。目前基于深度学习的行为识别算法效果已经超过了iDT算法，但与iDT的结果做ensemble总还是能获得一些提升。所以这几年好多论文的最优效果都是“Our method+iDT”的形式。这篇论文是iDT的基础。</p>
<h2 id="Dense-Trajectories密集轨迹算法"><a href="#Dense-Trajectories密集轨迹算法" class="headerlink" title="Dense Trajectories密集轨迹算法"></a>Dense Trajectories密集轨迹算法</h2><p>通过光流场获取视频序列轨迹，沿轨迹提取形状特征、HOF、HOG、MBH特恒，再利用BoF方法对特征编码，最后训练SVM分类器。主要分为三个阶段：密集特征采样、特征点轨迹跟踪、基于轨迹的特征提取。其主要结构为：</p>
<div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/DT.png" width="90%"><br></div>

<h3 id="Dense-Sampling密集采样"><a href="#Dense-Sampling密集采样" class="headerlink" title="Dense Sampling密集采样"></a>Dense Sampling密集采样</h3><p>在多空间尺度通过网格划分的方式密集采样特征点，多空间尺度保证了采样特征点覆盖所有空间位置及尺度。通常取8个空间尺度。后续的特征提取在各个尺度上分别进行，特征点采样间隔$W$大概为5。</p>
<p>在时间序列跟踪特征点时，需要去除部分无法跟踪的特征点。通过计算每个像素点的自相关矩阵的特征值，并设置阈值$T$去除部分特征值<br>$$<br>T = 0.001 \cdot max_{i \in I} min(\lambda _i^1, \lambda_i^2)<br>$$<br>其中$\lambda_i^1, \lambda_i^2$为像素点$i$的特征值。</p>
<h3 id="Trajectory-Shap-Descriptor轨迹形状描述子"><a href="#Trajectory-Shap-Descriptor轨迹形状描述子" class="headerlink" title="Trajectory Shap Descriptor轨迹形状描述子"></a>Trajectory Shap Descriptor轨迹形状描述子</h3><ol>
<li>Dense Sampling得到的密集采样后得到的某特征点坐标为$P_t = (x_t, y_t)$，则下一帧图像的特征点位置$P_{t+1} = (x_{t+1}, y_{t+1})$为：</li>
</ol>
<p>$$<br>P_{t+1} = (x_t, y_t) + (M * \omega_t)|_{x_t, y_t}<br>$$</p>
<p>$\omega_t = (u_t, v_t)$为密集光流场，由$I_t$，$I_{t+1}$计算得到</p>
<p>$M$为中值滤波器，大小为$3*3$</p>
<p>可以看出，计算特征点领域内的光流中值得到特征点的运动方向。</p>
<ol start="2">
<li>连续$L$帧可以使用$P_t$计算出后面的$P_{t+1},…,P_{t+L}$帧的轨迹，但长时跟踪不可靠，每$L$帧需要重新进行密集特征点采样，一般$L=15$。</li>
<li>轨迹本身也可以构成轨迹形状特征描述子。对于长度为$L$的轨迹，用$(\Delta P_t,…, \Delta P_{t+L-1})$描述，</li>
</ol>
<p>$$<br>\Delta P_t = (P_{t+1} - P_t) = (x_{t+1} - x_t, y_{t+1} - y_t)<br>$$</p>
<p>正则化后获得轨迹描述子$T$:<br>$$<br>T = \frac{(\Delta P_t,…, \Delta P_{t+L-1})}{\sum _{j = 1} ^{t+L-1} ||\Delta P_j||}<br>$$</p>
<h3 id="Motion-and-Structure-Descriptors运动和结构描述子"><a href="#Motion-and-Structure-Descriptors运动和结构描述子" class="headerlink" title="Motion and Structure Descriptors运动和结构描述子"></a>Motion and Structure Descriptors运动和结构描述子</h3><p>除了轨迹描述子$T$，还用到了HOG（Histogram of Gradient）、HOF（Histogram of Oriented Optical Flow）、MBH（Motion Boundary Histogram）等特征。</p>
<ol>
<li>沿着某特征点长度为$L$的轨迹，每帧图像取特征点周围$N \times N$区域（$N$=32），构成时空体（volume），对于时空体进行网格划分，空间上每个方向划分为$n_{\sigma}$份($n_{\sigma} = 2$)，时间上均匀选取$n_{\tau}$，($n_{\tau} = 2$)共分出$n_{\sigma} \times n_{\sigma} \times n_{\tau}$份。</li>
<li>对各个区域进行特征提取特征</li>
</ol>
<ul>
<li><strong>HOG</strong>特征：计算灰度图梯度直方图，bin_num=8，输出长度为$2<em>2</em>3*8$</li>
<li><strong>HOF</strong>特征：计算光流直方图（包括方向和幅度信息），bin_num=8+1，额外一个用于统计光流幅度小于某阈值的像素点，输出长度为$2<em>2</em>3*9$</li>
<li><strong>MBH</strong>特征：计算的是光流图像梯度的直方图，也可以理解为在光流图像上计算的HOG特征，由于光流图像包括x方向和y方向，故分别计算$MBH_x$和$MBH_y$，输出长度为$2<em>2</em>2<em>3</em>8$</li>
</ul>
<ol start="3">
<li>在计算完后，还需要进行特征的归一化，DT算法中对HOG,HOF和MBH均使用L2范数归一化。</li>
</ol>
<h3 id="Bag-of-Features特征编码"><a href="#Bag-of-Features特征编码" class="headerlink" title="Bag of Features特征编码"></a>Bag of Features特征编码</h3><p>对得到的特征组（T，HOG，HOF，MBH）编码，得到一个定长的编码。</p>
<p>Bag of Features模型仿照文本检索领域的Bag-of-Words方法</p>
<ul>
<li>把每幅图像描述为一个局部区域/关键点(Patches/Key Points)特征的无序集合。</li>
<li>使用某种聚类算法(如K-means)将局部特征进行聚类，每个聚类中心被看作是词典中的一个视觉词汇(Visual Word)，相当于文本检索中的词，视觉词汇由聚类中心对应特征形成的码字(code word)来表示（可看当为一种特征量化过程）。</li>
<li>所有视觉词汇形成一个视觉词典(Visual Vocabulary)，对应一个码书(code book)，即码字的集合，词典中所含词的个数反映了词典的大小。</li>
<li>图像中的每个特征都将被映射到视觉词典的某个词上，这种映射可以通过计算特征间的距离去实现，然后统计每个视觉词的出现与否或次数，图像可描述为一个维数相同的直方图向量，即Bag-of-Features。</li>
</ul>
<p>在训练码书时，DT算法随机选取了100000组特征进行训练。码书的大小则设置为4000。在训练完码书后，对每个视频的特征组进行编码，就可以得到视频对应的特征。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">NYY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2018/12/19/computer_version/DT/">http://yoursite.com/2018/12/19/computer_version/DT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/计算机视觉/">计算机视觉</a><a class="post-meta__tags" href="/tags/Action-Recognition/">Action Recognition</a><a class="post-meta__tags" href="/tags/论文笔记/">论文笔记</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/12/24/computer_version/scnn/"><i class="fa fa-chevron-left">  </i><span>scnn论文笔记</span></a></div><div class="next-post pull-right"><a href="/2018/12/18/computer_version/two-stream-conv/"><span>Two-Stream Convolutional Networks for Action Recognition in Videos论文笔记</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://yoursite.com/2018/12/19/computer_version/DT/';
  this.page.identifier = '2018/12/19/computer_version/DT/';
  this.page.title = 'DT算法论文笔记';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'niuyuanyuan' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-src" src="https://niuyuanyuan.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By NYY</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>