<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="TensorFlow实现Softmax Regression识别手写数字"><meta name="keywords" content="Tensorflow,入门教程,CNN"><meta name="author" content="NYY,undefined"><meta name="copyright" content="NYY"><title>TensorFlow实现Softmax Regression识别手写数字 | NYY's blog</title><link rel="shortcut icon" href="/img/my_icon.jpg"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#TensorFlow实现Softmax-Regression识别手写数字"><span class="toc-number">1.</span> <span class="toc-text">TensorFlow实现Softmax Regression识别手写数字</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#MNIST数据集"><span class="toc-number">1.1.</span> <span class="toc-text">MNIST数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Softmax-Regression算法"><span class="toc-number">1.2.</span> <span class="toc-text">Softmax Regression算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#输入和输出"><span class="toc-number">1.2.1.</span> <span class="toc-text">输入和输出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#损失函数"><span class="toc-number">1.2.2.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#随机梯度下降算法"><span class="toc-number">1.2.3.</span> <span class="toc-text">随机梯度下降算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练模型"><span class="toc-number">1.2.4.</span> <span class="toc-text">训练模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型评估"><span class="toc-number">1.3.</span> <span class="toc-text">模型评估</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">NYY</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/niuyuanyuanna" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">38</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">32</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">8</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://www.ouyangsong.com" target="_blank">欧阳松的博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://github.com/niuyuanyuanna/BlogImages/raw/master/background/tf.jpeg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">NYY's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">TensorFlow实现Softmax Regression识别手写数字</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-06</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Framework/">Framework</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/2018/12/06/tensorflow/2.2Softmax-Regression/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2018/12/06/tensorflow/2.2Softmax-Regression/"></span></a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1,932</span><span class="post-meta__separator">|</span><span>Reading time: 7 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="TensorFlow实现Softmax-Regression识别手写数字"><a href="#TensorFlow实现Softmax-Regression识别手写数字" class="headerlink" title="TensorFlow实现Softmax Regression识别手写数字"></a><font color="blue">TensorFlow实现Softmax Regression识别手写数字</font></h1><h2 id="MNIST数据集"><a href="#MNIST数据集" class="headerlink" title="MNIST数据集"></a>MNIST数据集</h2><p>MNIST（Mixed National Institute of Standards and Technology database）是一个非常简单的机器视觉数据集，它由几万张28像素×28像素的手写数字组成，这些图片只包含灰度值信息。我们的任务是对这些手写数字的图片进行分类，转换为0-9一共10类。<br></p>
<center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/tensorflow/62516735.jpg" width="50%/"><br></center>

<p>数据集包含：</p>
<ul>
<li>train-images-idx3-ubyte 训练数据图像 (60,000)</li>
<li>train-labels-idx1-ubyte 训练数据label</li>
<li>t10k-images-idx3-ubyte 测试数据图像 (10,000)</li>
<li>t10k-labels-idx1-ubyte 测试数据label<br><br>对MNIST数据集加载时，TensorFlow为我们提供了一个方便的封装，可以直接加载MNIST数据成为我们期望的格式。<br><br>对MNIST数据集加载时，TensorFlow为我们提供了一个方便的封装，可以直接加载MNIST数据成为我们期望的格式。<br></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import input_data</span><br><span class="line">mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)</span><br></pre></td></tr></table></figure>
<p>查看MNIST数据集，其图像是28×28像素大小的灰度图片。空白部分全为0，有笔迹的地方根据颜色深浅有0-1之间的取值。相当于每个样本有784（28×28=784）维特征，因此丢弃图片的二维结构信息，把一张图片变为一个很长的1维向量。<br></p>
<center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/tensorflow/90357367.jpg" width="75%/"><br></center>

<p>训练数据共有55000张灰度图，则训练集的训练数据为60000×784的Tensor，训练数据的Label是一个60000×10的Tensor。对10个类进行one-hot编码，Label是一个10维的向量，只有对应数字正确的位置为1，其他都为0.如数字0，对应Label为[1,0,0,0,0,0,0,0,0,0]；数字8对应Label为[0,0,0,0,0,0,0,0,1,0]。<br></p>
<h2 id="Softmax-Regression算法"><a href="#Softmax-Regression算法" class="headerlink" title="Softmax Regression算法"></a>Softmax Regression算法</h2><h3 id="输入和输出"><a href="#输入和输出" class="headerlink" title="输入和输出"></a>输入和输出</h3><p>&emsp;&emsp;我们的分类结果共有10个，当模型对一张图片进行预测时，Softmax Regression会对每一种类别估算一个概率。如预测是数字3的概率为80%，是数字5的概率为5%，最后概率最大的数字作为模型的输出。<br><br>&emsp;&emsp;当处理多分类任务时，通常需要使用Softmax Regression 模型。其工作原理很简单，可以判定为某类的特征相加，然后将这些特征转化为判定是这一类的概率。特征可以通过一些简单的方法得到，如对所有像素求加权和，权重是通过数据训练出来的。如某个像素的灰度值大代表很可能是数字n时，这个像素的权重就很大，如果某像素的灰度值大代表不太可能是数字n时，这个像素的权重可能是负的。<br>&emsp;     将这些特征写成如下公式：<br></p>
<p>$feature_i = \sum\limits_{j}W_{i,j}x_j+bb_j$</p>
<p>i代表第i类，j代表一张图片的第j个像素。bi是bias，即数据本身的一些偏置项，比如大部分数字都是0，那么0的特征对应的bias就会很大。<br><br>&emsp;&emsp;接下来对所有特征计算softmax。都是计算一个exp函数，再进行标准化，其目的是让所有的类别输出概率值和为1。$$softmax(x)=normalize(exp(x))$$其中判定为第i类的概率就可以由下面的公式得到：<br>    &emsp;$$softmax(x)_i=\frac{exp(x_i)}{\sum\limits_{j}exp(x_j)}$$<br>&emsp;&emsp;先对各个特征求exp函数，然后将其归一化，特征值越大的类，最后输出的概率也越大，使用exp函数保证了所有的特征值概率不可能小于或等于0，将Softmax Regression的计算过程可视化：<br></p>
<center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/tensorflow/38532106.jpg" width="75%/"><br></center>

<p>将图变为公式，元素相乘变为矩阵乘法：<br><br>$$<br>\begin{bmatrix}y_1\y_2\y_3\end{bmatrix}=softmax\left{\begin{bmatrix}W_{1,1}&amp;W_{1,2}&amp;W_{1,3}\W_{2,1}&amp;W_{2,2}&amp;W_{2,3}\W_{3,1}&amp;W_{3,2}&amp;W_{3,3}\end{bmatrix}\cdot\begin{bmatrix}x_1\x_2\x_3\end{bmatrix}+\begin{bmatrix}b_1\b_2\b_3\end{bmatrix}\right}<br>$$</p>
<p>则上述矩阵可以表示为：<br></p>
<p>$$y=softmax(Wx+b)$$<br>&emsp;&emsp;使用TensorFlow实现Softmax Regression。在python中使用Numpy进行矩阵的操作运算，首先创建一个输入的变量：<br></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">x = tf.placeholder(&quot;float&quot;, [None, 784])</span><br></pre></td></tr></table></figure>
<p>None表示样本的个数，这里不限个数的输入。<br></p>
<p>接下来创建weights和biases变量，并使用公式计算y的值<br></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">W = tf.Variable(tf.zeros([784,10]))</span><br><span class="line">b = tf.Variable(tf.zeros([10]))</span><br><span class="line">y = tf.nn.softmax(tf.matmul(x,W) + b)</span><br></pre></td></tr></table></figure>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>为了训练模型，需要定义一个loss function来描述模型对问题的分类精度。为了得到最好的训练效果，希望loss越小越好。对于多分类问题，通常使用交叉熵（cross-entropy）作为损失函数：$$H_{y’}(y)=-\sum\limits_{i}y’_i\log(y_i)$$其中y是预测的概率分布，y’是真实的概率分布。在TensorFlow中定义cross-entropy：<br><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y_ = tf.placeholder(&quot;float&quot;, [None,10])</span><br><span class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y))</span><br></pre></td></tr></table></figure></p>
<h3 id="随机梯度下降算法"><a href="#随机梯度下降算法" class="headerlink" title="随机梯度下降算法"></a>随机梯度下降算法</h3><p>TensorFlow可以根据定义好的计算题自动求导，并根据反向传播算法进行训练，在每一轮迭代时，更新参数来最小化loss。TensorFlow已经封装好了随机梯度下降算法的优化器，我们只需要直接调用<code>tf.train.GradientDescentOptimizer</code>，设置函数的学习率以及优化的目标损失函数。<br><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)</span><br></pre></td></tr></table></figure></p>
<p>在这里，我们要求TensorFlow用梯度下降算法（gradient descent algorithm）以0.01的学习速率最小化交叉熵。梯度下降算法（gradient descent algorithm）是一个简单的学习过程，TensorFlow只需将每个变量一点点地往使成本不断降低的方向移动。<br></p>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><p>定义好所有参数以及优化方法后，在Session中启动模型，并且初始化变量。<br><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">init = tf.initialize_all_variables()</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure></p>
<p>然后开始训练模型，这里我们让模型循环训练1000次。在每次训练时，随机抓取训练数据中的100个样本批处理样本点。<br><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for i in range(1000):</span><br><span class="line">    batch_xs, batch_ys = mnist.train.next_batch(100)</span><br><span class="line">    sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br></pre></td></tr></table></figure></p>
<p>使用一小部分的随机数据来进行训练被称为随机训练（stochastic training），更确切的说是随机梯度下降训练。在理想情况下，我们希望用所有的数据来进行每一步的训练，因为这能得到更好的训练结果，但这需要很大的计算开销。所以，每一次训练使用不同的数据子集，这样做既可以减少计算开销，又可以最大化地学习到数据集的总体特性。<br></p>
<h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><p>我们可以用预测结果的正确率来判断模型的性能。首先找出那些预测正确的标签。<code>tf.argmax</code>能给出某个tensor对象在某一维上数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如<code>tf.argmax(y,1)</code>返回的是模型对于任一输入x预测到的标签值，而<code>tf.argmax(y_,1)</code>代表实际的正确标签，用<code>tf.equal</code>来检测预测是否和真实标签匹配(索引位置一样表示匹配)。<br><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))</span><br></pre></td></tr></table></figure></p>
<p>最后，计算训练的模型在测试数据集上的正确率：<br><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)</span><br></pre></td></tr></table></figure></p>
<p>最终的结果值大约为91%，其正确率并不是很高，原因在于我们使用的模型十分简单，它并不是一个神经网络模型，在现在的模型基础上增加一层隐藏层，就会变成一个神经网络，且正确率更高。<br></p>
<p>备注：</p>
<p>完整的代码保存在”/home/student/public/deep_learning/TensorFlow/class2“中。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">NYY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2018/12/06/tensorflow/2.2Softmax-Regression/">http://yoursite.com/2018/12/06/tensorflow/2.2Softmax-Regression/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Tensorflow/">Tensorflow</a><a class="post-meta__tags" href="/tags/入门教程/">入门教程</a><a class="post-meta__tags" href="/tags/CNN/">CNN</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/12/06/tensorflow/3.1CNNforMNIST/"><i class="fa fa-chevron-left">  </i><span>使用深度CNN识别MNIST数据集</span></a></div><div class="next-post pull-right"><a href="/2018/12/06/tensorflow/2.1create_neural_network/"><span>TensorFlow搭建神经网络</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://yoursite.com/2018/12/06/tensorflow/2.2Softmax-Regression/';
  this.page.identifier = '2018/12/06/tensorflow/2.2Softmax-Regression/';
  this.page.title = 'TensorFlow实现Softmax Regression识别手写数字';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'niuyuanyuan' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-src" src="https://niuyuanyuan.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 By NYY</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>