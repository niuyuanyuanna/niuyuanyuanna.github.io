<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="TensorFlow基础入门"><meta name="keywords" content="Tensorflow,入门教程"><meta name="author" content="NYY"><meta name="copyright" content="NYY"><title>TensorFlow基础入门 | NYY's blog</title><link rel="shortcut icon" href="/img/my_icon.jpg"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#tensorflow基础入门"><span class="toc-number">1.</span> <span class="toc-text">TensorFlow基础入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tensorflow简介"><span class="toc-number">1.1.</span> <span class="toc-text">TensorFlow简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#基础使用"><span class="toc-number">1.2.</span> <span class="toc-text">基础使用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#构建图"><span class="toc-number">1.2.0.1.</span> <span class="toc-text">构建图</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#变量"><span class="toc-number">1.2.1.</span> <span class="toc-text">变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fetch"><span class="toc-number">1.2.2.</span> <span class="toc-text">Fetch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feed"><span class="toc-number">1.2.3.</span> <span class="toc-text">Feed</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">NYY</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/niuyuanyuanna">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">48</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">37</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">10</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://www.ouyangsong.com">欧阳松的博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://github.com/niuyuanyuanna/BlogImages/raw/master/background/tf.jpeg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">NYY's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">TensorFlow基础入门</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-06</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Framework/">Framework</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/2018/12/06/tensorflow/1.TensorFlow入门/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2018/12/06/tensorflow/1.TensorFlow入门/"></span></a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1,734</span><span class="post-meta__separator">|</span><span>Reading time: 6 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="tensorflow基础入门"><font color="blue">TensorFlow基础入门</font></h1>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><h2 id="tensorflow简介">TensorFlow简介</h2>
   TensorFlow是一个采用数据流图（data flow graphs），用于数值计算的开源软件库。最初由Google大脑小组（隶属于Google机器智能研究机构）的研究员和工程师们开发出来，用于机器学习和深度神经网络方面的研究，但这个系统的通用性使其也可广泛用于其他计算领域。<br>   TensorFlow既是一个实现机器学习算法的接口，同时也是执行机器学习速算法的框架。它前端支持Python、C++、Go、Java等多种开发语言，后端使用C++、CUDA等写成，其实现的算法可以在众多异构系统移植，如CPU，GPU集群，iOS，Android等。<br>
<center>
<img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/tensorflow/20181206185930.png" width="20%">
</center>
<p>  TensorFlow中的计算可以表示为一个有向图（directed graph），或计算图（computation graph），其中每一个运算操作将作为一个节点，称之为op（operation缩写），节点与节点之间的连接称为边（edge）。在计算图的边中流动（flow）的数据被称&gt;为张量（tensor），因此得名为TensorFlow。每个张量是一个类型化的多维数组。例如,可以将一小组图像集表示为一个四维浮点数数组,这四个维度分别是[batch, height, width, channels]。<br></p>
<h2 id="基础使用">基础使用</h2>
<p>  一个TensorFlow图描述了计算的过程。为了进行计算，图必须在会话里被启动。会话将图的op分发到诸如CPU或GPU之类的设备上，同时提供执行op的方法。这些方法执行后，将产生的tensor返回。在Python语言中，返回的tensor是numpy ndarray对象。<br><br>
### 计算图<br>
  TensorFlow程序通常被组织成一个构建阶段和一个执行阶段。在构建阶段，op的执行步骤被描述成一个图。在执行阶段，使用会话执行执行图中的op。例如，通常在构建阶段创建一个图来表示和训练神经网络，然后在执行阶段反复执行图中的训练op。<br></p>
<h4 id="构建图">构建图</h4>
<p>  构建图的第一步，是创建源op(source op)。源op不需要任何输入，例如常量(Constant)。源op的输出被传递给其它op做运算。Python库中，op构造器的返回值代表被构造出的op的输出，这些返回值可以传递给其它op构造器作为输入。TensorFlow Python库有一个默认图(default graph)，op构造器可以为其增加节点。这个默认图对许多程序来说已经足够用了。<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf                                                          </span><br><span class="line">                                                                                 </span><br><span class="line"># 创建一个常量op，产生一个1x2矩阵。这个op被作为一个节点                          </span><br><span class="line"># 加到默认图中.                                                                  </span><br><span class="line">#                                                                                </span><br><span class="line"># 构造器的返回值代表该常量op的返回值.                                            </span><br><span class="line">matrix1 = tf.constant([[3., 3.]])                                                </span><br><span class="line">                                                                                 </span><br><span class="line"># 创建另外一个常量op,产生一个2x1矩阵.                                            </span><br><span class="line">matrix2 = tf.constant([[2.],[2.]])                                               </span><br><span class="line">                                                                                 </span><br><span class="line"># 创建一个矩阵乘法 matmul op，把&apos;matrix1&apos;和&apos;matrix2&apos;作为输入。                   </span><br><span class="line"># 返回值&apos;product&apos;代表矩阵乘法的结果.                                             </span><br><span class="line">product = tf.matmul(matrix1, matrix2)</span><br></pre></td></tr></table></figure></p>
<p>  默认图现在有三个节点，两个constant() op，和一个matmul() op。为了真正进行矩阵相乘运算，并得到矩阵乘法的结果，必须在会话里启动这个图。<br><br>
#### 启动图<br>
  构造阶段完成后，才能启动图。启动图的第一步是创建一个Session对象，如果无任何创建参数，会话构造器将启动默认图。<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动默认图.                                                                    </span><br><span class="line">sess = tf.Session()                                                              </span><br><span class="line">                                                                                 </span><br><span class="line"># 调用 sess 的 &apos;run()&apos; 方法来执行矩阵乘法 op, 传入 &apos;product&apos; 作为该方法的参数.   </span><br><span class="line"># 上面提到, &apos;product&apos; 代表了矩阵乘法 op 的输出, 传入它是向方法表明, 我们希望取回 </span><br><span class="line"># 矩阵乘法 op 的输出.                                                            </span><br><span class="line">#                                                                                </span><br><span class="line"># 整个执行过程是自动化的, 会话负责传递 op 所需的全部输入. op 通常是并发执行的.   </span><br><span class="line">#                                                                                </span><br><span class="line"># 函数调用 &apos;run(product)&apos; 触发了图中三个 op (两个常量 op 和一个矩阵乘法 op) 的执行.</span><br><span class="line">#                                                                                </span><br><span class="line"># 返回值 &apos;result&apos; 是一个 numpy `ndarray` 对象.                                   </span><br><span class="line">result = sess.run(product)                                                       </span><br><span class="line">print result                                                                     </span><br><span class="line"># ==&gt; [[ 12.]]                                                                   </span><br><span class="line">                                                                                 </span><br><span class="line"># 任务完成, 关闭会话.                                                            </span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure></p>
<p>  Session对象在使用完后需要关闭以释放资源。除了显式调用close外，也可以使用“with”代码块来自动完成关闭动作。<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with tf.Session() as sess:                                                       </span><br><span class="line">result = sess.run([product])                                                     </span><br><span class="line">print result</span><br></pre></td></tr></table></figure></p>
<h3 id="变量">变量</h3>
<p>  变量维护图执行过程中的状态信息。下面的例子演示了如何使用变量实现一个简单的计数器。<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建一个变量, 初始化为标量 0.                                                  </span><br><span class="line">state = tf.Variable(0, name=&quot;counter&quot;)                                           </span><br><span class="line"># 创建一个 op, 其作用是使 state 增加 1     </span><br><span class="line">                                                                                 </span><br><span class="line">one = tf.constant(1)                                                             </span><br><span class="line">new_value = tf.add(state, one)                                                   </span><br><span class="line">update = tf.assign(state, new_value)                                             </span><br><span class="line">                                                                                 </span><br><span class="line"># 启动图后, 变量必须先经过`初始化` (init) op 初始化,                             </span><br><span class="line"># 首先必须增加一个`初始化` op 到图中.                                            </span><br><span class="line">init_op = tf.initialize_all_variables()                                          </span><br><span class="line">                                                                                 </span><br><span class="line"># 启动图, 运行 op                                                                </span><br><span class="line">with tf.Session() as sess:                                                       </span><br><span class="line">  # 运行 &apos;init&apos; op                                                               </span><br><span class="line">  sess.run(init_op)                                                              </span><br><span class="line">  # 打印 &apos;state&apos; 的初始值                                                        </span><br><span class="line">  print sess.run(state)                                                          </span><br><span class="line">  # 运行 op, 更新 &apos;state&apos;, 并打印 &apos;state&apos;                                        </span><br><span class="line">  for _ in range(3):                                                             </span><br><span class="line">    sess.run(update)                                                             </span><br><span class="line">    print sess.run(state)                                                        </span><br><span class="line">                                                                                 </span><br><span class="line"># 输出:                                                                          </span><br><span class="line">                                                                                 </span><br><span class="line"># 0                                                                              </span><br><span class="line"># 1                                                                              </span><br><span class="line"># 2                                                                              </span><br><span class="line"># 3</span><br></pre></td></tr></table></figure></p>
<p>  代码中assign()操作是图所描绘的表达式的一部分，正如add()操作一样。所以在调用run()执行表达式之前，它并不会真正执行赋值操作。<br>   通常会将一个统计模型中的参数表示为一组变量。例如，可以将一个神经网络的权重作为某个变量存储在一个tensor中。在训练过程中，通过重复运行训练图，更新这个tensor。<br></p>
<h3 id="fetch">Fetch</h3>
<p>  为了取回操作的输出内容，可以在使用Session对象的run()。调用执行图时，传入一些tensor，这些tensor会帮助你取回结果。在之前的例子里，只取回了单个节点state，但是也可以取回多个tensor:<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input1 = tf.constant(3.0)                                                        </span><br><span class="line">input2 = tf.constant(2.0)          </span><br><span class="line">intermed = tf.add(input2, input3)                                                </span><br><span class="line">mul = tf.mul(input1, intermed)                                                   </span><br><span class="line">                                                                                 </span><br><span class="line">with tf.Session():                                                               </span><br><span class="line">  result = sess.run([mul, intermed])                                             </span><br><span class="line">  print result                                                                   </span><br><span class="line">                                                                                 </span><br><span class="line"># 输出:                                                                          </span><br><span class="line"># [array([ 21.], dtype=float32), array([ 7.], dtype=float32)]</span><br></pre></td></tr></table></figure></p>
<p>  需要获取的多个tensor值，在op的一次运行中一起获得（而不是逐个去获取tensor）。<br></p>
<h3 id="feed">Feed</h3>
<p>  上述示例在计算图中引入了tensor，以常量或变量的形式存储。TensorFlow还提供了feed机制，该机制可以临时替代图中的任意操作中的tensor可以对图中任何操作提交补丁，直接插入一个tensor。<br>   feed使用一个tensor值临时替换一个操作的输出结果。可以提供feed数据作为<code>run()</code>调用的参数。feed只在调用它的方法内有效，方法结束，feed就会消失。最常见的用例是将某些特殊的操作指定为“feed”操作，标记的方法是使用<code>tf.placeholder()</code>为这些操作创建占位符。<br> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input1 = tf.placeholder(tf.types.float32)                                        </span><br><span class="line">input2 = tf.placeholder(tf.types.float32)                                        </span><br><span class="line">output = tf.mul(input1, input2)                                                  </span><br><span class="line">                                                                                 </span><br><span class="line">with tf.Session() as sess:                                                       </span><br><span class="line">  print sess.run([output], feed_dict=&#123;input1:[7.], input2:[2.]&#125;)                 </span><br><span class="line">                                                                                 </span><br><span class="line"># 输出:                                                                          </span><br><span class="line"># [array([ 14.], dtype=float32)]</span><br></pre></td></tr></table></figure></p>
<p>  如果没有正确提供feed，<code>placeholder()</code>操作将会产生错误。 <br></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">NYY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2018/12/06/tensorflow/1.TensorFlow入门/">http://yoursite.com/2018/12/06/tensorflow/1.TensorFlow入门/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Tensorflow/">Tensorflow</a><a class="post-meta__tags" href="/tags/入门教程/">入门教程</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/12/06/tensorflow/1.1TensorFlow_basic_exercise_one/"><i class="fa fa-chevron-left">  </i><span>TensorFlow基础实验一</span></a></div><div class="next-post pull-right"><a href="/2018/12/05/computer_version/tradition-surf/"><span>传统图像特征点提取算法</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://yoursite.com/2018/12/06/tensorflow/1.TensorFlow入门/';
  this.page.identifier = '2018/12/06/tensorflow/1.TensorFlow入门/';
  this.page.title = 'TensorFlow基础入门';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'niuyuanyuan' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-scr" src="https://niuyuanyuan.disqus.com/count.js" async></script></div></div><footer class="footer-bg" style="background-image: url(https://github.com/niuyuanyuanna/BlogImages/raw/master/background/tf.jpeg)"><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By NYY</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>