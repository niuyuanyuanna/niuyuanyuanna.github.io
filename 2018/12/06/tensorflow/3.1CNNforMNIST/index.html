<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="使用深度CNN识别MNIST数据集"><meta name="keywords" content="Tensorflow,入门教程,CNN"><meta name="author" content="NYY,undefined"><meta name="copyright" content="NYY"><title>使用深度CNN识别MNIST数据集 | NYY's blog</title><link rel="shortcut icon" href="/img/my_icon.jpg"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#使用深度CNN识别MNIST数据集"><span class="toc-number">1.</span> <span class="toc-text">使用深度CNN识别MNIST数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#准备工作"><span class="toc-number">1.1.</span> <span class="toc-text">准备工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#创建骨干程序"><span class="toc-number">1.1.1.</span> <span class="toc-text">创建骨干程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#加载数据集"><span class="toc-number">1.1.2.</span> <span class="toc-text">加载数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN介绍"><span class="toc-number">1.1.3.</span> <span class="toc-text">CNN介绍</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#深度CNN分类器"><span class="toc-number">1.2.</span> <span class="toc-text">深度CNN分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#创建分类器"><span class="toc-number">1.2.1.</span> <span class="toc-text">创建分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#输入层"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">输入层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#卷积层1"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">卷积层1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#池化层1"><span class="toc-number">1.2.1.3.</span> <span class="toc-text">池化层1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#卷积层2"><span class="toc-number">1.2.1.4.</span> <span class="toc-text">卷积层2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#池化层2"><span class="toc-number">1.2.1.5.</span> <span class="toc-text">池化层2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#全连接层1"><span class="toc-number">1.2.1.6.</span> <span class="toc-text">全连接层1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Dropout正则化"><span class="toc-number">1.2.1.7.</span> <span class="toc-text">Dropout正则化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Logits层"><span class="toc-number">1.2.1.8.</span> <span class="toc-text">Logits层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#生成预测"><span class="toc-number">1.2.1.9.</span> <span class="toc-text">生成预测</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#计算损失"><span class="toc-number">1.2.2.</span> <span class="toc-text">计算损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型训练"><span class="toc-number">1.2.3.</span> <span class="toc-text">模型训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型评估"><span class="toc-number">1.2.4.</span> <span class="toc-text">模型评估</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#训练模型过程"><span class="toc-number">1.3.</span> <span class="toc-text">训练模型过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#创建模型评估器"><span class="toc-number">1.3.1.</span> <span class="toc-text">创建模型评估器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#添加输出到日志系统"><span class="toc-number">1.3.2.</span> <span class="toc-text">添加输出到日志系统</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练过程"><span class="toc-number">1.3.3.</span> <span class="toc-text">训练过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型评估-1"><span class="toc-number">1.4.</span> <span class="toc-text">模型评估</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">NYY</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/niuyuanyuanna" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">42</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">32</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">8</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://www.ouyangsong.com" target="_blank">欧阳松的博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://github.com/niuyuanyuanna/BlogImages/raw/master/background/tf.jpeg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">NYY's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">使用深度CNN识别MNIST数据集</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-06</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Framework/">Framework</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/2018/12/06/tensorflow/3.1CNNforMNIST/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2018/12/06/tensorflow/3.1CNNforMNIST/"></span></a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">4,014</span><span class="post-meta__separator">|</span><span>Reading time: 14 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="使用深度CNN识别MNIST数据集"><a href="#使用深度CNN识别MNIST数据集" class="headerlink" title="使用深度CNN识别MNIST数据集"></a><font color="blue">使用深度CNN识别MNIST数据集</font></h1><p>TensorFlow是一个非常强大的用来做大规模数值计算的库。其所擅长的任务之一就是实现以及训练深度神经网络。<br>在本教程中，我们将学到构建一个TensorFlow模型的基本步骤，并将通过这些步骤为MNIST构建一个深度卷积神经网络。</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>在创建模型之前，需要先加载MNIST数据集，然后启动一个TensorFlow的session。MNIST数据集在上一堂课程中已经具体介绍过，这里不再赘述。</p>
<h3 id="创建骨干程序"><a href="#创建骨干程序" class="headerlink" title="创建骨干程序"></a>创建骨干程序</h3><p>为主程序设置一个骨干程序。创建一个名为<code>cnn_mnist.py</code>的文件，添加下列代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from __future__ import absolute_import</span><br><span class="line">from __future__ import division</span><br><span class="line">from __future__ import print_function</span><br><span class="line"></span><br><span class="line"># Imports</span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line"></span><br><span class="line"># Our application logic will be added here</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">  tf.app.run()</span><br></pre></td></tr></table></figure></p>
<p>完成本实验时，需要添加代码来构建，训练和评估卷积神经网络。完整的最终代码可以在 <a href="https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/examples/tutorials/layers/cnn_mnist.py" target="_blank" rel="noopener">这里</a>找到。</p>
<h3 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h3><p>为了方便起见，使用一个脚本来自动下载和导入<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST</a>数据集。它会自动创建一个<code>MNIST_data</code>的目录来存储数据。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def loadData():</span><br><span class="line">    mnist = input_data.read_data_sets(&quot;./tmp/data&quot;, one_hot=True)</span><br><span class="line">    train_data = mnist.train.images # 返回 np.array</span><br><span class="line">    train_labels = np.asarray(mnist.train.labels, dtype=np.int32)</span><br><span class="line">    test_data = mnist.test.images # 返回 np.array</span><br><span class="line">    test_labels = np.asarray(mnist.test.labels, dtype=np.int32)</span><br><span class="line">    return mnist, train_data, train_labels, test_data, test_labels</span><br></pre></td></tr></table></figure>
<p>这里，<code>mnist</code>是一个轻量级的类。它以Numpy数组的形式存储着训练、校验和测试数据集。同时提供了一个函数，用于在迭代中获得minibatch，后面我们将会用到。</p>
<h3 id="CNN介绍"><a href="#CNN介绍" class="headerlink" title="CNN介绍"></a>CNN介绍</h3><p>卷积神经网络（CNN）是当前用于图像分类任务的最先进的模型体系结构。CNN将一系列<code>filter</code>应用于图像的原始像素数据以提取和学习更高级别的特征，使得该模型可用于分类。CNN包含三个模块：</p>
<ul>
<li>卷积层。将特定数量的卷积滤镜应用于图像。对于每个子区域，图层执行一组卷积运算，在输出特征映射中生成单个值。卷积层通常将 ReLU激活函数应用于输出从而将非线性引入到模型中。</li>
<li>池化层。对由卷积层提取的图像数据进行下采样，减少特征映射的维度，从而减少处理时间。常用的池化算法是最大池化，对其提取特征的子区域（例如，2×2像素的块），保持其最大值并丢弃所有其他值。</li>
<li>全连接层。对由卷积图层提取的特征执行分类，并由池化层进行下采样。全连接层中，图层中的每个节点都连接到前一图层中的每个节点。</li>
</ul>
<p>通常，CNN由执行特征提取的多个卷积模块组成。每个模块由一个卷积层和一个池层组成。最后的卷积模块之后是一个或多个执行分类的全连接层。CNN最后的全连接层包含模型中每个目标类的单个节点（模型可能预测的所有可能的类），使用 <code>softmax</code>激活函数为每个节点生成0-1之间的值（softmax值之和等于1）。可以将给定图像的softmax值解释为图像落入每个目标类别可能性的相对测量值。</p>
<h2 id="深度CNN分类器"><a href="#深度CNN分类器" class="headerlink" title="深度CNN分类器"></a>深度CNN分类器</h2><p>在上一个实验中，我们采用softmax regression训练了一个多分类器，其在MINIST上仅有91%的正确率，识别效果差强人意，因此，此次实验将其扩展为一个拥有多层卷积网络的softmax回归模型，将准确率提升至97.07%左右。模型构造为：</p>
<ol>
<li>卷积层1：使用32个5x5滤波器（提取5x5像素子区域），具有ReLU激活函数；</li>
<li>池化层1：使用2x2过滤器和步长2执行最大池化（指定池与池不重叠）；</li>
<li>卷积层2：使用64个5x5滤波器，具有ReLU激活函数；</li>
<li>池化层2：使用2x2过滤器和步长2执行最大池化；</li>
<li>全连接层1：1,024个神经元，Dropout正则化率为0.4（概率为0.4，任何给定元素在训练期间将被丢弃）；</li>
<li>全连接层2（Logits Layer）：10个神经元，每个数字目标类别（0-9）一个。</li>
</ol>
<p><code>tf.layers</code>模块包含创建上述三种图层类型的方法：</p>
<ul>
<li><code>conv2d()</code>。构造一个二维卷积层。采用过滤器数量，过滤内核大小，填充和激活函数作为参数。</li>
<li><code>max_pooling2d()</code>。使用max-pooling算法构造一个二维池化层。采用过滤器大小和步幅作为参数。</li>
<li><code>dense()</code>。构建一个全连接层。以神经元数量和激活函数作为参数。<br>这些方法中的每一个都接受张量作为输入，并将变换后的张量作为输出返回。这样可以很容易地将一个图层连接到另一个图层：只需从一个图层创建方法获取输出并将其作为输入提供给另一个图层。<h3 id="创建分类器"><a href="#创建分类器" class="headerlink" title="创建分类器"></a>创建分类器</h3>创建<code>cnn_model_fn()</code>函数，函数符合TensorFlow的Estimator API预期的界面（稍后在创建估算器中的更多内容）。cnn_mnist.py取MNIST特征数据，标签和 模型模式（TRAIN，EVAL，PREDICT）作为参数; 配置CNN; 并返回预测，损失和培训操作：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def cnn_model_fn(features, labels, mode):</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>创建模型的相关操作都在这个函数中编写。</p>
<h4 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h4><p><code>layers</code>用于为二维图像数据创建卷积层和合并层的模块中的方法期望输入张量具有如下定义的形状 ：<code>[batch_size, image_width, image_height, channels]</code></p>
<ul>
<li><code>batch_size</code>：在训练期间执行梯度下降时使用的示例子集的大小。</li>
<li><code>image_width</code>：示例图像的宽度。</li>
<li><code>image_height</code>：示例图像的高度。</li>
<li><code>channels</code>：示例图像中的颜色通道数量。对于彩色图像，通道数量是3（红色，绿色，蓝色）。对于单色图像，只有1个通道（黑色）。<br>这里，MNIST数据集由单色的28x28像素图像组成，因此输入图层的所需形状为：<code>[batch_size, 28, 28, 1]</code><br>为了将我们的输入特征映射（features）转换为这种形状，我们可以执行以下reshape操作：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input_layer = tf.reshape(features[&quot;x&quot;], [-1,28,28,1])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>请注意，我们已经指定<code>-1</code>为批量大小，它指定此维度应根据输入值的数量进行动态计算 <code>features[&quot;x&quot;]</code>，并保持所有其他维度的大小不变。这使我们可以把它batch_size当作一个我们可以调整的超参数。例如，如果我们将样例以5批次的形式提供给我们的模型，<code>features[&quot;x&quot;]</code>将包含3,920个值（每个图像中的每个像素都有一个值），并且<code>input_layer</code>的形状为<code>[5, 28, 28, 1]</code>。同样，如果我们以100个批次为例提供示例，<code>features[&quot;x&quot;]</code>将包含78,400个值，并且<code>input_layer</code>的形状为<code>[100, 28, 28, 1]</code>。</p>
<h4 id="卷积层1"><a href="#卷积层1" class="headerlink" title="卷积层1"></a>卷积层1</h4><p>现在可以开始实现第一层。它由一个卷积接一个max pooling组成。在第一个卷积层中，我们希望将32个5x5滤波器应用到输入层，并使用ReLU激活函数。可以使用模块中<code>layers</code>的<code>conv2d()</code>方法来创建此图层，如下所示：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conv1 = tf.layers.conv2d(</span><br><span class="line">    inputs=input_layer,</span><br><span class="line">    filters=32,</span><br><span class="line">    kernel_size=[5, 5],</span><br><span class="line">    padding=&quot;same&quot;,</span><br><span class="line">    activation=tf.nn.relu)</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>inputs</code>参数指定了输入张量，形状为 ：<code>[batch_size, image_width, image_height, channels]</code>。在这里，将第一个卷积层连接到<code>input_layer</code>；</li>
<li><code>filters</code>参数指定过滤器的数量，这里是32；</li>
<li><code>kernel_size</code>指定了作为过滤器的尺寸<code>[width, height]</code>，这里是<code>[5, 5]</code>；</li>
<li><code>padding</code>参数指定两个枚举值之一（不区分大小写）：<code>valid</code>（默认值）或<code>same</code>。为了指定输出张量应该与输入张量具有相同的宽度和高度值，在这里设置padding=same，它表示TensorFlow将0值添加到输入张量的边缘以保持宽度和高度为28.（没有填充，a在28x28的张量上进行5x5的卷积将产生形状为24x24的张量）；</li>
<li><code>activation</code>参数指定应用于卷积输出的激活函数。在这里，我们指定了使用ReLU激活 <code>tf.nn.relu</code>。<br>输出张量<code>conv2d()</code>的形状为 ：<code>[batch_size, 28, 28, 32]</code>，与输入相同的宽度和高度尺寸，但现在有32个通道保持每个滤波器的输出。</li>
</ul>
<h4 id="池化层1"><a href="#池化层1" class="headerlink" title="池化层1"></a>池化层1</h4><p>接下来，将第一个池化层连接到刚刚创建的卷积层。可以使用<code>layers</code>的<code>max_pooling2d()</code>方法来构建一个使用2x2过滤器和步长为2的最大池化层：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>inputs</code>指定输入张量，形状为 ：<code>[batch_size, image_width, image_height, channels]</code>。在这里，将第一个卷积层的输出连接到<code>inputs</code>；</li>
<li><code>pool_size</code>表示最大池化过滤器的尺寸<code>[width, height]</code>，这里是<code>[2, 2]</code>；</li>
<li><code>strides</code>指定步长的大小。在这里，我们设置步长为2，这表明由滤波器提取的子区域应该在宽度和高度维度上分开2个像素（对于2x2滤波器，这意味着没有提取的区域将重叠）。如果要为宽度和高度设置不同的步长值，则可以改为指定元组或列表（例如，<code>stride=[3, 6]</code>）。<br>由<code>max_pooling2d()</code>（pool1）生成的输出张量具有以下形状 ：<code>[batch_size, 14, 14, 32]</code>，2x2滤波器每个将宽度和高度减小50％。</li>
</ul>
<h4 id="卷积层2"><a href="#卷积层2" class="headerlink" title="卷积层2"></a>卷积层2</h4><p>类似于第一个卷积层，创建第二个卷积层，不过此处需要将其输入变为第一个池化层的输出。改变滤波器的个数，配置64个滤波器，尺寸不变：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conv2 = tf.layers.conv2d(</span><br><span class="line">    inputs=pool1,</span><br><span class="line">    filters=64,</span><br><span class="line">    kernel_size=[5, 5],</span><br><span class="line">    padding=&quot;same&quot;,</span><br><span class="line">    activation=tf.nn.relu)</span><br></pre></td></tr></table></figure>
<h4 id="池化层2"><a href="#池化层2" class="headerlink" title="池化层2"></a>池化层2</h4><p>和第一个池化层类似，只需要改变其输入为第二个卷积层的输出其步长和滤波器尺寸均不变：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)</span><br></pre></td></tr></table></figure></p>
<h4 id="全连接层1"><a href="#全连接层1" class="headerlink" title="全连接层1"></a>全连接层1</h4><p>现在，图片尺寸减小到7x7，最后加入一个有1024个神经元的全连接层，用于处理整个图片。把池化层输出的张量<code>reshape</code>成一些向量，再使用<code>dense()</code>方法来连接全连接层。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pool2_flat = tf.reshape(pool2, [-1, 7*7*64])</span><br><span class="line">dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>units</code>指定全连接层中的神经元数（1024）</li>
<li><code>activation</code>表示可使用的激活函数，使用<code>tf.nn.relu</code>添加ReLU激活。</li>
</ul>
<h4 id="Dropout正则化"><a href="#Dropout正则化" class="headerlink" title="Dropout正则化"></a>Dropout正则化</h4><p>为了减少过拟合，在输出层之前加入dropout。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dropout = tf.layers.dropout(</span><br><span class="line">    inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)</span><br></pre></td></tr></table></figure></p>
<ul>
<li><code>inputs</code>指定输入张量，它是来自全连接层1（<code>dense</code>）的输出张量；</li>
<li><code>rate</code>指定了丢弃概率; 在这里，我们使用0.4，这意味着40％的元素将在训练中被随机丢弃；</li>
<li><code>training</code>使用布尔指定模型目前是否训练模式下运行; 如果<code>training</code>为<code>True</code>，则仅执行丢弃。在这里，我们检查mode传递的模型函数<code>cnn_model_fn</code>是否是TRAIN模式。</li>
</ul>
<p>输出张量<code>dropout</code>的形状为：<code>[batch_size, 1024]</code>。</p>
<h4 id="Logits层"><a href="#Logits层" class="headerlink" title="Logits层"></a>Logits层</h4><p>神经网络中的最后一层是logits层，它会返回预测的原始值。创建一个包含10个神经元（每个目标类为0-9）的全连接层，并使用ReLU激活函数（默认值）：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">logits = tf.layers.dense(inputs=dropout, units=10)</span><br></pre></td></tr></table></figure></p>
<p>最终的CNN输出张量<code>logits</code>形状为：<code>[batch_size, 10]</code>。</p>
<h4 id="生成预测"><a href="#生成预测" class="headerlink" title="生成预测"></a>生成预测</h4><p>对于一个给定的例子，预测的类别是具有最高原始值的对数张量对应行中的元素。可以使用<code>tf.argmax</code> 函数找到这个元素的索引：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.argmax(input=logits, axis=1)</span><br></pre></td></tr></table></figure></p>
<p>通过softmax激活函数从<code>logits</code>层中得出该样本属于不同类别的概率，使用tf.nn.softmax：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.nn.softmax(logits, name=&quot;softmax_tensor&quot;)</span><br></pre></td></tr></table></figure></p>
<p>用字典存储预测值，并返回一个<code>EstimatorSpec</code>对象：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">predictions = &#123;</span><br><span class="line">    &quot;classes&quot;: tf.argmax(input=logits, axis=1),</span><br><span class="line">    &quot;probabilities&quot;: tf.nn.softmax(logits, name=&quot;softmax_tensor&quot;)</span><br><span class="line">&#125;</span><br><span class="line">if mode == tf.estimator.ModeKeys.PREDICT:</span><br><span class="line">  return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)</span><br></pre></td></tr></table></figure></p>
<h3 id="计算损失"><a href="#计算损失" class="headerlink" title="计算损失"></a>计算损失</h3><p>对于训练和评估，需要定义一个损失函数来衡量模型的预测和目标类别的匹配度。对于MNIST多分类问题，通常采用交叉熵作为损失度量。以下代码计算模型以任一模式<code>TRAIN</code>或<code>EVAL</code>模式运行时的交叉熵：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)</span><br><span class="line">loss = tf.losses.softmax_cross_entropy(</span><br><span class="line">    onehot_labels=onehot_labels, logits=logits)</span><br></pre></td></tr></table></figure></p>
<p><code>labels</code>张量包含了样本的预测列表，例如[1, 9, …]。为了计算交叉熵，首先需要转换<code>labels</code> 成相应的单热编码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],</span><br><span class="line"> [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],</span><br><span class="line"> ...]</span><br></pre></td></tr></table></figure></p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>在前面，将CNN的损失定义为<code>logits</code>层和标签的<code>softmax</code>交叉熵。现在配置模以在训练期间优化这个损失值。使用0.001的学习率和随机梯度下降算法作为优化算法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)</span><br><span class="line">  train_op = optimizer.minimize(</span><br><span class="line">      loss=loss,</span><br><span class="line">      global_step=tf.train.get_global_step())</span><br><span class="line">  return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)</span><br></pre></td></tr></table></figure></p>
<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>要在模型中添加准确性度量，<code>eval_metric_ops</code>在<code>EVAL</code>模式中定义字典，如下所示：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">eval_metric_ops = &#123;</span><br><span class="line">    &quot;accuracy&quot;: tf.metrics.accuracy(</span><br><span class="line">        labels=labels, predictions=predictions[&quot;classes&quot;])&#125;</span><br><span class="line">return tf.estimator.EstimatorSpec(</span><br><span class="line">    mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)</span><br></pre></td></tr></table></figure></p>
<h2 id="训练模型过程"><a href="#训练模型过程" class="headerlink" title="训练模型过程"></a>训练模型过程</h2><h3 id="创建模型评估器"><a href="#创建模型评估器" class="headerlink" title="创建模型评估器"></a>创建模型评估器</h3><p>创建<code>main()</code>函数，将下面的步骤加入<code>main()</code>函数中。</p>
<p>创建一个<code>Estimator</code>来评估模型（TensorFlow类，用于执行高级模型训练，评估和推理）。将以下代码添加到main()：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Create the Estimator</span><br><span class="line">mnist_classifier = tf.estimator.Estimator(</span><br><span class="line">    model_fn=cnn_model_fn, model_dir=&quot;./tmp/mnist_convnet_model&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>model_fn</code>指定模型用于训练、评估或预测，传递<code>cnn_model_fn</code>，即分类器函数；</li>
<li><code>model_dir</code>指定模型数据的保存目录，这里指定一个临时目录，可以更改为需要存储的路径。</li>
</ul>
<h3 id="添加输出到日志系统"><a href="#添加输出到日志系统" class="headerlink" title="添加输出到日志系统"></a>添加输出到日志系统</h3><p>由于CNN可能需要一段时间才能进行训练，因此设置一些日志记录，以便在训练期间跟踪进度。可以使用<code>tf.train.SessionRunHook</code>创建一个 <code>tf.train.LoggingTensorHook</code>，记录softmax层的概率值。将以下内容添加到main()：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Set up logging for predictions</span><br><span class="line">tensors_to_log = &#123;&quot;probabilities&quot;: &quot;softmax_tensor&quot;&#125;</span><br><span class="line">logging_hook = tf.train.LoggingTensorHook(</span><br><span class="line">    tensors=tensors_to_log, every_n_iter=50)</span><br></pre></td></tr></table></figure></p>
<p><code>tensors_to_log</code>是一个字典，用于存储需要输出日志的张量。字典的<code>key</code>是选择的标签，将打印在日志输出中，相应的<code>value</code>是TensorTensorFlow图模型中的张量名称。</p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>现在，已经准备好训练模型，可以通过创建<code>train_input_fn</code>和调用<code>mnist_classifier</code>的<code>train()</code>函数进行训练。将以下内容添加到<code>main()</code>函数：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Train the model</span><br><span class="line">train_input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;&quot;x&quot;: train_data&#125;,</span><br><span class="line">    y=train_labels,</span><br><span class="line">    batch_size=100,</span><br><span class="line">    num_epochs=None,</span><br><span class="line">    shuffle=True)</span><br><span class="line">mnist_classifier.train(</span><br><span class="line">    input_fn=train_input_fn,</span><br><span class="line">    steps=20000,</span><br><span class="line">    hooks=[logging_hook])</span><br></pre></td></tr></table></figure></p>
<p>在<code>numpy_input_fn</code>函数中：</p>
<ul>
<li>分别将训练特征数据和标签传递给<code>x</code>（作为字典）<code>y</code>；</li>
<li>设置<code>batch_size</code>的100（每一步在100个样本中训练模型）；</li>
<li><code>num_epochs=None</code>表示模型将训练到达到指定的步数；</li>
<li><p><code>shuffle=True</code>表示洗牌训练数据。<br>在<code>mnist_classifier.train</code>函数中：</p>
</li>
<li><p>设置<code>steps=20000</code> （模型将训练共20,000步）；</p>
</li>
<li>设置<code>hooks</code>为<code>logging_hook</code>，它在训练期间被触发。<br>由于运行输出数据较多，只选取其中一部分展示：</li>
</ul>
<center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/tensorflow/47979028.jpg" width="75%"><br></center>

<h2 id="模型评估-1"><a href="#模型评估-1" class="headerlink" title="模型评估"></a>模型评估</h2><p>训练完成后，需要评估模型以确定其在MNIST测试集上的准确性。使用<code>evaluate</code>方法评估模型，和训练模型类似，但只需要将迭代步数改变为1，即只需要执行一次操作。将以下内容添加到main()：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Evaluate the model and print results</span><br><span class="line">eval_input_fn = tf.estimator.inputs.numpy_input_fn(</span><br><span class="line">    x=&#123;&quot;x&quot;: eval_data&#125;,</span><br><span class="line">    y=eval_labels,</span><br><span class="line">    num_epochs=1,</span><br><span class="line">    shuffle=False)</span><br><span class="line">eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)</span><br><span class="line">print(eval_results)</span><br></pre></td></tr></table></figure></p>
<p>运行结果：</p>
<center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/tensorflow/30440377.jpg" width="75%"><br></center>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INFO:tensorflow:Saving checkpoints for 20000 into /tmp/mnist_convnet_model/model.ckpt.                                                                                                      </span><br><span class="line">INFO:tensorflow:Loss for final step: 0.0446627.                                                                                                                                             </span><br><span class="line">INFO:tensorflow:Starting evaluation at 2018-04-18-11:59:47                                                                                                                                  </span><br><span class="line">2018-04-18 11:59:47.857775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Quadro P4000, pci bus id: 0000:01:00.0,</span><br><span class="line"> compute capability: 6.1)                                                                                                                                                                   </span><br><span class="line">INFO:tensorflow:Restoring parameters from /tmp/mnist_convnet_model/model.ckpt-20000                                                                                                         </span><br><span class="line">INFO:tensorflow:Finished evaluation at 2018-04-18-11:59:48                                                                                                                                  </span><br><span class="line">INFO:tensorflow:Saving dict for global step 20000: accuracy = 0.9707, global_step = 20000, loss = 0.100675                                                                                  </span><br><span class="line">&#123;&apos;loss&apos;: 0.10067523, &apos;global_step&apos;: 20000, &apos;accuracy&apos;: 0.97070003&#125;</span><br></pre></td></tr></table></figure>
<p>备注：</p>
<p>完整的代码保存在”/home/student/public/deep_learning/TensorFlow/class3“中。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">NYY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2018/12/06/tensorflow/3.1CNNforMNIST/">http://yoursite.com/2018/12/06/tensorflow/3.1CNNforMNIST/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Tensorflow/">Tensorflow</a><a class="post-meta__tags" href="/tags/入门教程/">入门教程</a><a class="post-meta__tags" href="/tags/CNN/">CNN</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/12/06/tensorflow/4.1迁移学习识别花/"><i class="fa fa-chevron-left">  </i><span>Inception-V3迁移学习</span></a></div><div class="next-post pull-right"><a href="/2018/12/06/tensorflow/2.2Softmax-Regression/"><span>TensorFlow实现Softmax Regression识别手写数字</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://yoursite.com/2018/12/06/tensorflow/3.1CNNforMNIST/';
  this.page.identifier = '2018/12/06/tensorflow/3.1CNNforMNIST/';
  this.page.title = '使用深度CNN识别MNIST数据集';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'niuyuanyuan' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-src" src="https://niuyuanyuan.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By NYY</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>