<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="TensorFlow搭建神经网络"><meta name="keywords" content="Tensorflow,入门教程,CNN"><meta name="author" content="NYY,undefined"><meta name="copyright" content="NYY"><title>TensorFlow搭建神经网络 | NYY's blog</title><link rel="shortcut icon" href="/img/my_icon.jpg"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#TensorFlow搭建神经网络"><span class="toc-number">1.</span> <span class="toc-text"> TensorFlow搭建神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#实验目的"><span class="toc-number">1.1.</span> <span class="toc-text">实验目的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实验内容"><span class="toc-number">1.2.</span> <span class="toc-text">实验内容</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#导入TensorFlow库"><span class="toc-number">1.2.1.</span> <span class="toc-text">导入TensorFlow库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据集"><span class="toc-number">1.2.2.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#创建placeholders"><span class="toc-number">1.2.3.</span> <span class="toc-text">创建placeholders</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#初始化参数变量"><span class="toc-number">1.2.4.</span> <span class="toc-text">初始化参数变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#前向传播"><span class="toc-number">1.2.5.</span> <span class="toc-text">前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#代价函数"><span class="toc-number">1.2.6.</span> <span class="toc-text">代价函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#反向传播"><span class="toc-number">1.2.7.</span> <span class="toc-text">反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#创建模型"><span class="toc-number">1.2.8.</span> <span class="toc-text">创建模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#测试自己的图片"><span class="toc-number">1.2.9.</span> <span class="toc-text">测试自己的图片</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">NYY</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/niuyuanyuanna" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">46</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">34</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://www.ouyangsong.com" target="_blank">欧阳松的博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://github.com/niuyuanyuanna/BlogImages/raw/master/background/tf.jpeg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">NYY's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">TensorFlow搭建神经网络</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-06</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Framework/">Framework</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/2018/12/06/tensorflow/2.1create_neural_network/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2018/12/06/tensorflow/2.1create_neural_network/"></span></a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2,779</span><span class="post-meta__separator">|</span><span>Reading time: 11 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="TensorFlow搭建神经网络"><a href="#TensorFlow搭建神经网络" class="headerlink" title=" TensorFlow搭建神经网络"></a><font color="blue"> TensorFlow搭建神经网络</font></h1><h2 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h2><p>使用TensorFlow搭建一个神经网络。在此次实验中，实现TensorFlow模型有两部分</p>
<ul>
<li>创建计算图</li>
<li>运行计算图</li>
</ul>
<h2 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h2><h3 id="导入TensorFlow库"><a href="#导入TensorFlow库" class="headerlink" title="导入TensorFlow库"></a>导入TensorFlow库</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import math</span><br><span class="line">import numpy as np</span><br><span class="line">import h5py</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.python.framework import ops</span><br><span class="line">from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">np.random.seed(1)</span><br></pre></td></tr></table></figure>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>此次使用的数据集为手语图片，你需要构建一个算法，以便于从语言障碍人士到不懂手语的人进行沟通。</p>
<ul>
<li>训练集：1080张手势图片，每张图片像素为（64×64），手势共有6类，分别表示从0到5的数字（每种有180张图片）</li>
<li>测试集：120张手势图片，每张图片像素为（64×64），手势共有6类，分别表示从0到5的数字（每种有20张图片）</li>
</ul>
<p>请注意，这是SINGS数据集的一个子集，完整的数据集包含更多的图像。<br>以下是每个数字的示例，以及如何表示标签。 这些在将图像的像素降低到64×64像素之前的原始图片。</p>
<p><center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/tensorflow/69143400.jpg" width="75%/"><br></center><br>下列代码用于加载数据集：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 加载数据集</span><br><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br></pre></td></tr></table></figure></p>
<p>更改下面代码中的index并运行代码，可视化数据集中的一个样本。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 图片样本</span><br><span class="line">index = 0</span><br><span class="line">plt.imshow(X_train_orig[index])</span><br><span class="line">print (&quot;y = &quot; + str(np.squeeze(Y_train_orig[:, index])))</span><br></pre></td></tr></table></figure></p>
<p>代码运行结果为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y = 5</span><br></pre></td></tr></table></figure></p>
<p>将数据集平坦化，然后通过对数据矩阵除以255对其进行归一化。并且将标签转换为one-hot编码的向量，运行下面的代码执行此操作。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 平坦化数据集</span><br><span class="line">X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T</span><br><span class="line">X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T</span><br><span class="line"># 归一化数据集</span><br><span class="line">X_train = X_train_flatten/255.</span><br><span class="line">X_test = X_test_flatten/255.</span><br><span class="line"># 将标签转换为one-hot编码向量</span><br><span class="line">Y_train = convert_to_one_hot(Y_train_orig, 6)</span><br><span class="line">Y_test = convert_to_one_hot(Y_test_orig, 6)</span><br><span class="line"></span><br><span class="line">print (&quot;number of training examples = &quot; + str(X_train.shape[1]))</span><br><span class="line">print (&quot;number of test examples = &quot; + str(X_test.shape[1]))</span><br><span class="line">print (&quot;X_train shape: &quot; + str(X_train.shape))</span><br><span class="line">print (&quot;Y_train shape: &quot; + str(Y_train.shape))</span><br><span class="line">print (&quot;X_test shape: &quot; + str(X_test.shape))</span><br><span class="line">print (&quot;Y_test shape: &quot; + str(Y_test.shape))</span><br></pre></td></tr></table></figure></p>
<p>代码运行结果为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">number of training examples = 1080</span><br><span class="line">number of test examples = 120</span><br><span class="line">X_train shape: (12288, 1080)</span><br><span class="line">Y_train shape: (6, 1080)</span><br><span class="line">X_test shape: (12288, 120)</span><br><span class="line">Y_test shape: (6, 120)</span><br></pre></td></tr></table></figure></p>
<p>注意：12288=64×64×3。每个图片为64×64像素，3是RGB颜色。<br>目的：搭建一个高精度识别手势的算法。要做到这一点，需要建立一个TensorFlow模型，最后使用softmax层输出。<br>模型：LINEAR  - &gt; RELU  - &gt; LINEAR  - &gt; RELU  - &gt; LINEAR  - &gt; SOFTMAX。</p>
<h3 id="创建placeholders"><a href="#创建placeholders" class="headerlink" title="创建placeholders"></a>创建placeholders</h3><p>首先为X和Y创建placeholder，在运行session时传递训练数据。<br>练习：补充下面的函数，在TensorFlow中创建placeholder。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># GRADED FUNCTION: create_placeholders</span><br><span class="line"></span><br><span class="line">def create_placeholders(n_x, n_y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    为session创建placeholder</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    n_x -- scalar, 图片向量的大小 (num_px * num_px = 64 * 64 * 3 = 12288)</span><br><span class="line">    n_y -- scalar, 类别数量 (从0到5的数字, 因此为6)</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    X -- 输入数据集的placeholder, shape：[n_x, None] ，dtype：&quot;float&quot;</span><br><span class="line">    Y -- 输出标签的placeholder, shape：[n_y, None] ，dtype：&quot;float&quot;</span><br><span class="line">    </span><br><span class="line">    Tips:</span><br><span class="line">    - 创建placeholder时使用None可以灵活地处理placeholder的数量，</span><br><span class="line">      因为训练集和测试集的样本数量是不相同的。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    ### 填写代码 ###</span><br><span class="line">    X = None</span><br><span class="line">    Y = None</span><br><span class="line">    ### 完成代码 ### </span><br><span class="line">    </span><br><span class="line">    return X, Y</span><br></pre></td></tr></table></figure></p>
<p>检验：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X, Y = create_placeholders(12288, 6)</span><br><span class="line">print (&quot;X = &quot; + str(X))</span><br><span class="line">print (&quot;Y = &quot; + str(Y))</span><br></pre></td></tr></table></figure></p>
<p>代码运行结果为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X = Tensor(&quot;Placeholder:0&quot;, shape=(12288, ?), dtype=float32)</span><br><span class="line">Y = Tensor(&quot;Placeholder_1:0&quot;, shape=(6, ?), dtype=float32)</span><br></pre></td></tr></table></figure></p>
<h3 id="初始化参数变量"><a href="#初始化参数变量" class="headerlink" title="初始化参数变量"></a>初始化参数变量</h3><p>接下来需要在TensorFlow中初始化参数。<br>练习：实现下面的功能来初始化tensorflow中的参数。使用Xavier初始化权重，Zero 初始化偏置项。参数的形状如下。给出例子，W1和B1：</p>
<ul>
<li>W1 = tf.get_variable(“W1”, [25,12288], initializer =tf.contrib.layers.xavier_initializer(seed = 1))</li>
<li>b1 = tf.get_variable(“b1”, [25,1], initializer = tf.zeros_initializer())</li>
</ul>
<p>为了确保所有人的输出值都相同，使用seed=1。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># GRADED FUNCTION: initialize_parameters</span><br><span class="line"></span><br><span class="line">def initialize_parameters():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    初始化神经网络的参数，其形状为:</span><br><span class="line">                        W1 : [25, 12288]</span><br><span class="line">                        b1 : [25, 1]</span><br><span class="line">                        W2 : [12, 25]</span><br><span class="line">                        b2 : [12, 1]</span><br><span class="line">                        W3 : [6, 12]</span><br><span class="line">                        b3 : [6, 1]</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- 存储W1, b1, W2, b2, W3, b3张量的字典</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    tf.set_random_seed(1)                   # 为了确保所有人的输出值都相同</span><br><span class="line">        </span><br><span class="line">    ### 填写代码 ###</span><br><span class="line">    W1 = None</span><br><span class="line">    b1 = None</span><br><span class="line">    W2 = None</span><br><span class="line">    b2 = None</span><br><span class="line">    W3 = None</span><br><span class="line">    b3 = None</span><br><span class="line">    ### 完成代码 ### </span><br><span class="line"></span><br><span class="line">    parameters = &#123;&quot;W1&quot;: W1,</span><br><span class="line">                  &quot;b1&quot;: b1,</span><br><span class="line">                  &quot;W2&quot;: W2,</span><br><span class="line">                  &quot;b2&quot;: b2,</span><br><span class="line">                  &quot;W3&quot;: W3,</span><br><span class="line">                  &quot;b3&quot;: b3&#125;</span><br><span class="line">    </span><br><span class="line">    return parameters</span><br></pre></td></tr></table></figure></p>
<p>测试代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    print(&quot;W1 = &quot; + str(parameters[&quot;W1&quot;]))</span><br><span class="line">    print(&quot;b1 = &quot; + str(parameters[&quot;b1&quot;]))</span><br><span class="line">    print(&quot;W2 = &quot; + str(parameters[&quot;W2&quot;]))</span><br><span class="line">    print(&quot;b2 = &quot; + str(parameters[&quot;b2&quot;]))</span><br></pre></td></tr></table></figure></p>
<p>代码运行结果为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">W1 = &lt;tf.Variable &apos;W1:0&apos; shape=(25, 12288) dtype=float32_ref&gt;</span><br><span class="line">b1 = &lt;tf.Variable &apos;b1:0&apos; shape=(25, 1) dtype=float32_ref&gt;</span><br><span class="line">W2 = &lt;tf.Variable &apos;W2:0&apos; shape=(12, 25) dtype=float32_ref&gt;</span><br><span class="line">b2 = &lt;tf.Variable &apos;b2:0&apos; shape=(12, 1) dtype=float32_ref&gt;</span><br></pre></td></tr></table></figure></p>
<p>此时参数还未进行赋值。</p>
<h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>现在实现TensorFlow前向传播的模块。该函数将接收参数字典并完成正向传播。可以使用的函数是：</p>
<ul>
<li>tf.add(…,…) 矩阵加法</li>
<li>tf.matmul(…,…) 矩阵乘法</li>
<li>tf.nn.relu(…) ReLu激活函数</li>
</ul>
<p>问题：实现神经网络的正向传播。将numpy和TensorFlow进行比较。注意前向传播在z3处停止，因为在TensorFlow中，最后的线性层输出作为损失函数的输入。因此，不需要计算a3.<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># GRADED FUNCTION: forward_propagation</span><br><span class="line"></span><br><span class="line">def forward_propagation(X, parameters):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    前向传播模型: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X -- input dataset placeholder, of shape (input size, number of examples)</span><br><span class="line">    parameters -- python字典，包含 &quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;, &quot;W3&quot;, &quot;b3&quot;</span><br><span class="line">                  其形状由initialize_parameters函数给出</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    Z3 -- 最后一个线性层的输出</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # 从&quot;parameters&quot;字典中恢复参数</span><br><span class="line">    W1 = parameters[&apos;W1&apos;]</span><br><span class="line">    b1 = parameters[&apos;b1&apos;]</span><br><span class="line">    W2 = parameters[&apos;W2&apos;]</span><br><span class="line">    b2 = parameters[&apos;b2&apos;]</span><br><span class="line">    W3 = parameters[&apos;W3&apos;]</span><br><span class="line">    b3 = parameters[&apos;b3&apos;]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">### 填写代码 ###              # Numpy 等式:</span><br><span class="line">    Z1 = None                                              # Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = None                                              # A1 = relu(Z1)</span><br><span class="line">    Z2 = None                                              # Z2 = np.dot(W2, a1) + b2</span><br><span class="line">    A2 = None                                              # A2 = relu(Z2)</span><br><span class="line">    Z3 = None                                              # Z3 = np.dot(W3,Z2) + b3</span><br><span class="line">    ### 完成代码 ### </span><br><span class="line">    </span><br><span class="line">    return Z3</span><br></pre></td></tr></table></figure></p>
<p>代码运行结果为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Z3 = Tensor(&quot;add_2:0&quot;, shape=(6, ?), dtype=float32)</span><br></pre></td></tr></table></figure></p>
<p>此时发现前向传播不会输出任何值，在反向传播的过程中可以知道原因。</p>
<h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>在之前的实验中，代价函数可以根据一行代码给出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ..., labels = ...))</span><br></pre></td></tr></table></figure></p>
<p>问题：补充代价函数</p>
<ul>
<li>tf.nn.softmax_cross_entropy_with_logits的“logits”和“labels”输入具有的形状应为（样本数，类别数），这里提供已经变形后的Z3和Y</li>
<li>tf.reduce_mean是对样本进行求和</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># GRADED FUNCTION: compute_cost </span><br><span class="line"></span><br><span class="line">def compute_cost(Z3, Y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    计算损失</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    Z3 -- 前向传播得到的输出 (模型最后一层线性层的输出), 形状为 (6, 样本数)</span><br><span class="line">    Y -- 标签向量placeholder, 和Z3有相同的形状</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    cost - 代价函数的张量</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # 需要用到 tf.nn.softmax_cross_entropy_with_logits(...,...)</span><br><span class="line">    logits = tf.transpose(Z3)</span><br><span class="line">    labels = tf.transpose(Y)</span><br><span class="line">    </span><br><span class="line">    ### 填写代码 ###   </span><br><span class="line">    cost = None</span><br><span class="line">    ### 完成代码 ### </span><br><span class="line">    </span><br><span class="line">    return cost</span><br></pre></td></tr></table></figure>
<p>测试代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    X, Y = create_placeholders(12288, 6)</span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    Z3 = forward_propagation(X, parameters)</span><br><span class="line">    cost = compute_cost(Z3, Y)</span><br><span class="line">    print(&quot;cost = &quot; + str(cost))</span><br></pre></td></tr></table></figure></p>
<p>代码运行结果为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cost = Tensor(&quot;Mean:0&quot;, shape=(), dtype=float32)</span><br></pre></td></tr></table></figure></p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>使用TensorFlow时，所有反向传播的参数更新都使用1行代码处理。计算了代价函数后，创建一个“optimizer”对象，运行tf.session时，对象必须和代价函数同时运行。当它被调用时，优化给定的方法和学习率的代价函数。例如，使用梯度下降法优化代价函数：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)</span><br></pre></td></tr></table></figure></p>
<p>为了运行这个对象，需要：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">_ , c = sess.run([optimizer, cost], feed_dict=&#123;X: minibatch_X, Y: minibatch_Y&#125;)</span><br></pre></td></tr></table></figure></p>
<p>在计算图中，从成本到输入，反向地计算，来更新参数值。<br>注意：在编码时，通常使用“ _ ”作为一次性变量，存储以后不需要使用的值，c为代价的值。</p>
<h3 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h3><p>将上面的模块融合到一起就是一个神经网络的模型。<br>练习：创建model，使用上面完成的函数。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,</span><br><span class="line">          num_epochs = 1500, minibatch_size = 32, print_cost = True):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    创建一个三层的TensorFlow神经网络: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SOFTMAX.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    X_train -- 训练集，形状为： (输入特征 = 12288, 训练样本数 = 1080)</span><br><span class="line">    Y_train -- 训练集的标签，形状为 (输出标签 = 6, 训练样本数  = 1080)</span><br><span class="line">    X_test -- 测试集，形状为 (输入特征 = 12288, 测试样本数 =  120)</span><br><span class="line">    Y_test -- 测试集的标签，形状为 (输出标签 = 6, 测试样本数  =  120)</span><br><span class="line">    learning_rate -- 学习率</span><br><span class="line">    num_epochs -- 迭代次数</span><br><span class="line">    minibatch_size -- 小批量的大小</span><br><span class="line">    print_cost -- 每迭代100次打印一次代价值</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    parameters -- 模型学习得到的参数，可以直接用于预测。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ops.reset_default_graph()                         </span><br><span class="line">    tf.set_random_seed(1)                             # 便于产生相同的输出</span><br><span class="line">    seed = 3                                          # 便于产生相同的输出</span><br><span class="line">    (n_x, m) = X_train.shape                          # (n_x: 输入样本特征, m : 训练集样本数)</span><br><span class="line">    n_y = Y_train.shape[0]                            # n_y : 输出标签数</span><br><span class="line">    costs = []                                        # 记录cost</span><br><span class="line">    </span><br><span class="line">    # 创建形状为 (n_x, n_y)的placeholder</span><br><span class="line">    ### 填写代码 ###   </span><br><span class="line">    X, Y = None</span><br><span class="line">    ### 完成代码 ###   </span><br><span class="line"></span><br><span class="line">    # 初始化参数</span><br><span class="line">    ### 填写代码 ###   </span><br><span class="line">    parameters = None</span><br><span class="line">    ### 完成代码 ###   </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    # 前向传播: 创建前向传播计算图</span><br><span class="line">    ### 填写代码 ###   </span><br><span class="line">    Z3 = None</span><br><span class="line">    ### 完成代码 ###   </span><br><span class="line">    </span><br><span class="line">    # 代价函数: 在计算图中增加代价函数</span><br><span class="line">    ### 填写代码 ###   </span><br><span class="line">    cost = None</span><br><span class="line">    ### 完成代码 ###   </span><br><span class="line">    </span><br><span class="line">    # 反向传播: 使用 AdamOptimizer优化代价函数。</span><br><span class="line">    ### 填写代码 ###   </span><br><span class="line">    optimizer = None</span><br><span class="line">    ### 完成代码 ###   </span><br><span class="line">    </span><br><span class="line">    # 初始化所有变量</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">    # 运行session计算TensorFlow图</span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        </span><br><span class="line">        # 运行初始化</span><br><span class="line">        sess.run(init)</span><br><span class="line">        </span><br><span class="line">        # 循环训练</span><br><span class="line">        for epoch in range(num_epochs):</span><br><span class="line"></span><br><span class="line">            epoch_cost = 0.                       # 定义每次迭代的代价函数</span><br><span class="line">            num_minibatches = int(m / minibatch_size) # 训练集选择的小批量的大小</span><br><span class="line">            seed = seed + 1</span><br><span class="line">            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)</span><br><span class="line"></span><br><span class="line">            for minibatch in minibatches:</span><br><span class="line"></span><br><span class="line">                # 选择一个批量</span><br><span class="line">                (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line">                </span><br><span class="line">                # 运行session执行 &quot;optimizer&quot;最小化 &quot;cost&quot;</span><br><span class="line">                ### 填写代码 ### </span><br><span class="line">                _ , minibatch_cost = None</span><br><span class="line">                ### 完成代码 ### </span><br><span class="line">                </span><br><span class="line">                epoch_cost += minibatch_cost / num_minibatches</span><br><span class="line"></span><br><span class="line">            # 每100次迭代输出一次代价值</span><br><span class="line">            if print_cost == True and epoch % 100 == 0:</span><br><span class="line">                print (&quot;Cost after epoch %i: %f&quot; % (epoch, epoch_cost))</span><br><span class="line">            if print_cost == True and epoch % 5 == 0:</span><br><span class="line">                costs.append(epoch_cost)</span><br><span class="line">                </span><br><span class="line">        # 画出代价值随迭代次数变化的图</span><br><span class="line">        plt.plot(np.squeeze(costs))</span><br><span class="line">        plt.ylabel(&apos;cost&apos;)</span><br><span class="line">        plt.xlabel(&apos;iterations (per tens)&apos;)</span><br><span class="line">        plt.title(&quot;Learning rate =&quot; + str(learning_rate))</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">        # 存储参数变量</span><br><span class="line">        parameters = sess.run(parameters)</span><br><span class="line">        print (&quot;Parameters have been trained!&quot;)</span><br><span class="line"></span><br><span class="line">        # 计算正确率</span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))</span><br><span class="line"></span><br><span class="line">        # 计算测试集的正确率</span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, &quot;float&quot;))</span><br><span class="line"></span><br><span class="line">        print (&quot;Train Accuracy:&quot;, accuracy.eval(&#123;X: X_train, Y: Y_train&#125;))</span><br><span class="line">        print (&quot;Test Accuracy:&quot;, accuracy.eval(&#123;X: X_test, Y: Y_test&#125;))</span><br><span class="line">        </span><br><span class="line">        return parameters</span><br></pre></td></tr></table></figure></p>
<p>测试代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">parameters = model(X_train, Y_train, X_test, Y_test)</span><br></pre></td></tr></table></figure></p>
<p>代码运行结果：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Train Accuracy: 0.999074</span><br><span class="line">Test Accuracy: 0.725</span><br></pre></td></tr></table></figure></p>
<ul>
<li>模型看起来可以很好地训练数据集，但是，由于训练集和测试集的准确率之间差异过大，可以尝试添加L2或Dropout正则化来减少过拟合。</li>
<li>将Session看作是一个训练模型的代码块，每次都在小批量的训练数据上运行会话训练参数。训练过1500次之后可以得到很好的参数。</li>
</ul>
<h3 id="测试自己的图片"><a href="#测试自己的图片" class="headerlink" title="测试自己的图片"></a>测试自己的图片</h3><p>将自己想要测试的图片添加到“images”文件夹中，再运行下列代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import scipy</span><br><span class="line">from PIL import Image</span><br><span class="line">from scipy import ndimage</span><br><span class="line"></span><br><span class="line">### 填写代码 ### (PUT YOUR IMAGE NAME) </span><br><span class="line">my_image = &quot;thumbs_up.jpg&quot;</span><br><span class="line">### 完成代码 ### </span><br><span class="line"></span><br><span class="line">fname = &quot;images/&quot; + my_image</span><br><span class="line">image = np.array(ndimage.imread(fname, flatten=False))</span><br><span class="line">my_image = scipy.misc.imresize(image, size=(64,64)).reshape((1, 64*64*3)).T</span><br><span class="line">my_image_prediction = predict(my_image, parameters)</span><br><span class="line"></span><br><span class="line">plt.imshow(image)</span><br><span class="line">print(&quot;Your algorithm predicts: y = &quot; + str(np.squeeze(my_image_prediction)))</span><br></pre></td></tr></table></figure></p>
<p>总结：</p>
<ul>
<li><p>TensorFlow是用于深度学习的编程框架</p>
</li>
<li><p>TensorFlow中的两个主要对象类是张量和操作符。</p>
</li>
<li><p>在TensorFlow中编码时，必须采取以下步骤：</p>
<ul>
<li>创建一个包含张量（变量，占位符…）和操作（tf.matmul，tf.add，…）的图形</li>
<li>创建一个会话</li>
<li>初始化会话</li>
<li>运行会话以执行图形</li>
</ul>
</li>
<li><p>可以多次执行计算图，如本实验中的model()</p>
</li>
<li>在“优化器”对象上运行会话时，会自动完成反向传播和优化</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">NYY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2018/12/06/tensorflow/2.1create_neural_network/">http://yoursite.com/2018/12/06/tensorflow/2.1create_neural_network/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Tensorflow/">Tensorflow</a><a class="post-meta__tags" href="/tags/入门教程/">入门教程</a><a class="post-meta__tags" href="/tags/CNN/">CNN</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/12/06/tensorflow/2.2Softmax-Regression/"><i class="fa fa-chevron-left">  </i><span>TensorFlow实现Softmax Regression识别手写数字</span></a></div><div class="next-post pull-right"><a href="/2018/12/06/tensorflow/1.1TensorFlow_basic_exercise_one/"><span>TensorFlow基础实验一</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://yoursite.com/2018/12/06/tensorflow/2.1create_neural_network/';
  this.page.identifier = '2018/12/06/tensorflow/2.1create_neural_network/';
  this.page.title = 'TensorFlow搭建神经网络';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'niuyuanyuan' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-src" src="https://niuyuanyuan.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By NYY</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>