<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="TensorFlow基础实验一"><meta name="keywords" content="Tensorflow,入门教程"><meta name="author" content="NYY,undefined"><meta name="copyright" content="NYY"><title>TensorFlow基础实验一 | NYY's blog</title><link rel="shortcut icon" href="/img/my_icon.jpg"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#TensorFlow基础实验一"><span class="toc-number">1.</span> <span class="toc-text"> TensorFlow基础实验一</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#实验目的"><span class="toc-number">1.1.</span> <span class="toc-text">实验目的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实验内容"><span class="toc-number">1.2.</span> <span class="toc-text">实验内容</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TensorFlow-Library"><span class="toc-number">1.2.1.</span> <span class="toc-text">TensorFlow Library</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#session"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">session</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#placeholder"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">placeholder</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#线性函数"><span class="toc-number">1.2.2.</span> <span class="toc-text">线性函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sigmoid函数"><span class="toc-number">1.2.3.</span> <span class="toc-text">sigmoid函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#代价函数"><span class="toc-number">1.2.4.</span> <span class="toc-text">代价函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用one-hot编码"><span class="toc-number">1.2.5.</span> <span class="toc-text">使用one hot编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用0和1初始化"><span class="toc-number">1.2.6.</span> <span class="toc-text">使用0和1初始化</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">NYY</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/niuyuanyuanna" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">47</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">34</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://www.ouyangsong.com" target="_blank">欧阳松的博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://github.com/niuyuanyuanna/BlogImages/raw/master/background/tf.jpeg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">NYY's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">TensorFlow基础实验一</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-06</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Framework/">Framework</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/2018/12/06/tensorflow/1.1TensorFlow_basic_exercise_one/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2018/12/06/tensorflow/1.1TensorFlow_basic_exercise_one/"></span></a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2,380</span><span class="post-meta__separator">|</span><span>Reading time: 9 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="TensorFlow基础实验一"><a href="#TensorFlow基础实验一" class="headerlink" title=" TensorFlow基础实验一"></a><font color="blue"> TensorFlow基础实验一</font></h1><h2 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h2><p>通过学习TensorFlow基础入门，掌握TensorFlow的基本使用方法。在此次实验中，你需要掌握这些技巧：</p>
<ul>
<li>初始化变量</li>
<li>创建会话</li>
<li>训练算法</li>
<li>搭建一个神经网络</li>
</ul>
<h2 id="实验内容"><a href="#实验内容" class="headerlink" title="实验内容"></a>实验内容</h2><h3 id="TensorFlow-Library"><a href="#TensorFlow-Library" class="headerlink" title="TensorFlow Library"></a>TensorFlow Library</h3><p>为了使用TensorFlow，首先需要引入TensorFlow库。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import math</span><br><span class="line">import numpy as np</span><br><span class="line">import h5py</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.python.framework import ops</span><br><span class="line"></span><br><span class="line">np.random.seed(1)</span><br></pre></td></tr></table></figure></p>
<p>引入相应所需的库之后，可以使用TensorFlow实现不同的应用。首先从一个简单的例子开始，计算训练数据的损失。<br>$loss = \mathcal{L}(\hat{y}, y) = (\hat y^{(i)} - y^{(i)})^2 \tag{1} $</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">y_hat = tf.constant(36, name=&apos;y_hat&apos;)            # 定义 y_hat为常量，设置其值为36</span><br><span class="line">y = tf.constant(39, name=&apos;y&apos;)                    # 定义 y为常量，设置其值为39</span><br><span class="line"></span><br><span class="line">loss = tf.Variable((y - y_hat)**2, name=&apos;loss&apos;)  # 定义损失函数为变量</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()         # 初始化，在后面用到 (session.run(init))</span><br><span class="line">                                                 # 损失函数将会被初始化并加入计算图中</span><br><span class="line">with tf.Session() as session:                    # 创建session输出结果</span><br><span class="line">    session.run(init)                            # 初始化变量</span><br><span class="line">    print(session.run(loss))                     # 输出损失</span><br></pre></td></tr></table></figure>
<p>此段代码运行结果为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">9</span><br></pre></td></tr></table></figure></p>
<p>在TensorFlow中执行程序的一般步骤为：</p>
<ol>
<li>创建还未被执行的张量</li>
<li>写出张量之间的操作，如表达式</li>
<li>初始化张量</li>
<li>创建会话（Session）</li>
<li>运行会话，这时会运行之前写好的操作</li>
</ol>
<p>因此，当我们为损失创建一个变量时，我们简单地将损失定义为其他数量的函数，但没有计算它的值。为了计算它，我们必须运行init = tf.global_variables_initializer（）。初始化损失变量，并在最后一行中，我们终于能够计算损失的价并打印其值。</p>
<h4 id="session"><a href="#session" class="headerlink" title="session"></a>session</h4><p>再来看一个简单的例子。试运行下列代码：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a = tf.constant(2)</span><br><span class="line">b = tf.constant(10)</span><br><span class="line">c = tf.multiply(a,b)</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure></p>
<p>此段代码运行结果为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Tensor(&quot;Mul:0&quot;, shape=(), dtype=int32)</span><br></pre></td></tr></table></figure></p>
<p>这里并没有输出20，输出结果为一个张量，没有shape属性，且类型为“ini32”。当前你的操作只是将其放置在“计算图”中，并没有运行这个计算。为了能实际的进行运算，需要创建一个会话运行它。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(c))</span><br></pre></td></tr></table></figure></p>
<p>此段代码运行结果为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">20</span><br></pre></td></tr></table></figure></p>
<p>总结：记得初始化你的变量，创建一个会话并在会话中运行这些操作。</p>
<h4 id="placeholder"><a href="#placeholder" class="headerlink" title="placeholder"></a>placeholder</h4><p>接下来，需要了解placeholders。placeholder是一个对象，其值只能在稍后指定。要指定占placeholder的值，可以使用“feed字典”（feed_dict变量）传入值。下面，我们为x创建了一个placeholder。这允许我们稍后在运行会话时传入一个数字。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x = tf.placeholder(tf.int64, name = &apos;x&apos;)</span><br><span class="line">print(sess.run(2 * x, feed_dict = &#123;x: 3&#125;))</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure></p>
<p>此段代码运行结果为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">6</span><br></pre></td></tr></table></figure></p>
<p>当第一次定义x时，不必为它指定一个值。placeholder只是一个变量，您将在稍后运行会话时将数据分配给该变量。在运行会话时向这些placeholder提供数据。当指定计算所需的操作时，告诉TensorFlow如何构建计算图。计算图可以有一些placeholder，其值将在稍后指定。最后，当你运行会话时，告诉TensorFlow执行计算图。</p>
<h3 id="线性函数"><a href="#线性函数" class="headerlink" title="线性函数"></a>线性函数</h3><p>首先从简单的线性函数开始，计算这个等式：$Y = WX+b$，其中$W$和$X$都是随机取值的矩阵，$b$是一个数值随机的向量。<br>练习：计算$WX+b$，其中$W$、$X$和$b$取值于随机正态分布。$W$是4×3的矩阵，$X$是3×1的矩阵，$b$为4×1的向量。作为例子，给出定义一个3×1的常量矩阵$X$：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">X = tf.constant(np.random.randn(3,1), name = &quot;X&quot;)</span><br></pre></td></tr></table></figure></p>
<p>你可以使用这些函数：</p>
<ul>
<li>tf.matmul(…, …) 矩阵乘法</li>
<li>tf.add(…, …) 矩阵加法</li>
<li>np.random.randn(…) 随机初始化<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># GRADED FUNCTION: linear_function</span><br><span class="line"></span><br><span class="line">def linear_function():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    定义线性函数: </span><br><span class="line">            初始化W为一个4×3的随机张量</span><br><span class="line">            初始化X为一个3×1的随机张量</span><br><span class="line">            初始化b为一个4×1的随机张量</span><br><span class="line">    返回值: </span><br><span class="line">    result -- Y = WX + b </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    np.random.seed(1)</span><br><span class="line">    </span><br><span class="line">    ### 填写代码 ###</span><br><span class="line">    X = None</span><br><span class="line">    W = None</span><br><span class="line">    b = None</span><br><span class="line">    Y = None</span><br><span class="line">    ### 完成代码 ### </span><br><span class="line">    </span><br><span class="line">    # tf.Session()创建会话，使用sess.run(...)计算变量</span><br><span class="line">    </span><br><span class="line">    ### 填写代码 ###</span><br><span class="line">    sess = None</span><br><span class="line">    result = None</span><br><span class="line">    ### 完成代码 ### </span><br><span class="line">    </span><br><span class="line">    # 关闭session </span><br><span class="line">    sess.close()</span><br><span class="line"></span><br><span class="line">    return result</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print( &quot;result = &quot; + str(linear_function()))</span><br></pre></td></tr></table></figure>
<p>此段代码运行结果为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">result	[[-2.15657382] [ 2.95891446] [-1.08926781] [-0.84538042]]</span><br></pre></td></tr></table></figure></p>
<h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p>Tensorflow提供了各种常用的神经网络功能，如tf.sigmoid和tf.softmax。 对于这个练习，我们计算输入的S形函数。<br>使用placeholder变量x执行此练习。 运行会话时，应该使用feed字典传入输入z。 在本练习中，需要（i）创建占位符x，（ii）使用tf.sigmoid定义计算sigmoid所需的操作，然后（iii）运行会话。<br>练习：创建sigmoid函数，需要用到下面的函数：</p>
<ul>
<li>tf.placeholder(tf.float32, name = “…”)</li>
<li>tf.sigmoid(…)</li>
<li>sess.run(…, feed_dict = {x: z})<br>注意：有两种方法创建session<br>方法1：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line"># 运行变量初始化及相应运算操作</span><br><span class="line">result = sess.run(..., feed_dict = &#123;...&#125;)</span><br><span class="line">sess.close() # 关闭session</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>方法2：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with tf.Session() as sess: </span><br><span class="line">    # 运行变量初始化及相应运算操作</span><br><span class="line">    result = sess.run(..., feed_dict = &#123;...&#125;)</span><br><span class="line">    # 此方法可以自动关闭session</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># GRADED FUNCTION: sigmoid</span><br><span class="line"></span><br><span class="line">def sigmoid(z):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    计算z的sigmoid值</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    z -- 输入，可以为具体的数值或向量</span><br><span class="line">    </span><br><span class="line">    Returns: </span><br><span class="line">    results -- z的sigmoid值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### 填写代码 ### ( approx. 4 lines of code)</span><br><span class="line">    # 为创建一个placeholder为其命名为&apos;x&apos;.</span><br><span class="line">    x = None</span><br><span class="line"></span><br><span class="line">    # 计算sigmoid(x)</span><br><span class="line">    sigmoid = None</span><br><span class="line"></span><br><span class="line">    # 创建session并运行 </span><br><span class="line">    # 使用feed_dict将z的值传递给x </span><br><span class="line">    None</span><br><span class="line">        # 运行session</span><br><span class="line">        result = None</span><br><span class="line">    </span><br><span class="line">    ### 完成代码 ###</span><br><span class="line">    </span><br><span class="line">    return result</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print (&quot;sigmoid(0) = &quot; + str(sigmoid(0)))</span><br><span class="line">print (&quot;sigmoid(12) = &quot; + str(sigmoid(12)))</span><br></pre></td></tr></table></figure>
<p>此段代码运行结果为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sigmoid(0) = 0.5</span><br><span class="line">sigmoid(12) = 0.999994</span><br></pre></td></tr></table></figure></p>
<p>总结：需要掌握</p>
<ol>
<li>创建placeholder</li>
<li>指定要计算的操作所对应的计算图</li>
<li>创建会话</li>
<li>运行会话，必要时使用feed_dict来指定placeholder变量的值</li>
</ol>
<h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>在TensorFlow中可以使用内置函数来计算神经网络的代价函数值。不需要编写公式来实现代价函数。<br>$J = - \frac{1}{m}  \sum_{i = 1}^m  \large ( \small y^{(i)} \log a^{ [2] (i)} + (1-y^{(i)})\log (1-a^{ [2] (i)} )\large )\small\tag{2}$<br>在TensorFlow中仅需一行代码便可以实现上述功能。</p>
<p>练习：引入交叉熵损失，需要用到下面的函数：</p>
<ul>
<li>tf.nn.sigmoid_cross_entropy_with_logits(logits = …,  labels = …)</li>
</ul>
<p>代码应输入z，计算sigmoid，得到a值，然后计算交叉熵损失J。所有的操作都可以通过调用一次tf.nn.sigmoid_cross_entropy_with_logits完成，其计算过程为：<br>$- \frac{1}{m}  \sum_{i = 1}^m  \large ( \small y^{(i)} \log \sigma(z^{<a href="i">2</a>}) + (1-y^{(i)})\log (1-\sigma(z^{<a href="i">2</a>})\large )\small\tag{3}$</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># GRADED FUNCTION: cost</span><br><span class="line"></span><br><span class="line">def cost(logits, labels):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    使用交叉熵作为损失函数，并计算其值</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    logits -- 包含z的矢量，最后一个线性单元的输出（在最终的sigmoid激活之前）</span><br><span class="line">    labels -- vector of labels y (1 or 0) </span><br><span class="line">    </span><br><span class="line">  Notes：在这里“z”和“y”分别为“logits”和“labels”，在在TensorFlow文档中，logits将输入到z中，并标记为y。</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    cost -- 运行session计算得出的cost</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### 填写代码 ### </span><br><span class="line">    </span><br><span class="line">    # 为z和y创建placeholder</span><br><span class="line">    z = None</span><br><span class="line">    y = None</span><br><span class="line">    </span><br><span class="line">    # 计算损失函数</span><br><span class="line">    cost = None</span><br><span class="line">    </span><br><span class="line">    # 采用第一种方法创建session</span><br><span class="line">    sess = None</span><br><span class="line">    </span><br><span class="line">    # 运行session</span><br><span class="line">    cost = None</span><br><span class="line">    </span><br><span class="line">    # 采用第一种方法关闭session</span><br><span class="line">    None</span><br><span class="line">    </span><br><span class="line">    ### 完成代码 ###</span><br><span class="line">    </span><br><span class="line">    return cost</span><br></pre></td></tr></table></figure>
<h3 id="使用one-hot编码"><a href="#使用one-hot编码" class="headerlink" title="使用one hot编码"></a>使用one hot编码</h3><p>经过多次深度学习之后，得到的y向量会有C个值，且C是累=类的数量。如果C为4，那么可能得到以下的y向量，需要按照以下方式转换：</p>
<center><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/tensorflow/78291355.jpg" width="75%/"><br></center>

<p>这被称为“独热”编码，因为在转换的表示中，每列的一个元素恰恰是“热”（意思是设置为1）。要在numpy中进行这种转换，可能需要编写几行代码。在TensorFlow中，可以使用一行代码：</p>
<ul>
<li>tf.one_hot(labels, depth, axis)</li>
</ul>
<p>练习：执行下面的函数，取一个标签向量和CC类总数，返回一个热独编码。使用tf.one_hot（）来做到这一点。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># GRADED FUNCTION: one_hot_matrix</span><br><span class="line"></span><br><span class="line">def one_hot_matrix(labels, C):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    创建一个矩阵，其中第i行对应于第i个类，第j列对应于第j个训练样例。所以，如果例子j有一个标签i，则输入（i，j）的值为1。</span><br><span class="line">                     </span><br><span class="line">    Arguments:</span><br><span class="line">    labels -- 包含标签的向量</span><br><span class="line">    C -- 类别的数量，one-hot编码的维度</span><br><span class="line">    </span><br><span class="line">    Returns: </span><br><span class="line">    one_hot -- one hot矩阵</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### 填写代码 ###</span><br><span class="line">    </span><br><span class="line">    # Create a tf.constant equal to C (depth), name it &apos;C&apos;. (approx. 1 line)</span><br><span class="line">    C = None</span><br><span class="line">    </span><br><span class="line">    # Use tf.one_hot, be careful with the axis (approx. 1 line)</span><br><span class="line">    one_hot_matrix = None</span><br><span class="line">    </span><br><span class="line">    # Create the session (approx. 1 line)</span><br><span class="line">    sess = None</span><br><span class="line">    </span><br><span class="line">    # Run the session (approx. 1 line)</span><br><span class="line">    one_hot = None</span><br><span class="line">    </span><br><span class="line">    # Close the session (approx. 1 line). See method 1 above.</span><br><span class="line">    None</span><br><span class="line">    </span><br><span class="line">    ### 完成代码 ###</span><br><span class="line">    </span><br><span class="line">    return one_hot</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">labels = np.array([1,2,3,0,2,1])</span><br><span class="line">one_hot = one_hot_matrix(labels, C = 4)</span><br><span class="line">print (&quot;one_hot = &quot; + str(one_hot))</span><br></pre></td></tr></table></figure>
<p>此段代码运行结果为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">one_hot = [[ 0.  0.  0.  1.  0.  0.]</span><br><span class="line"> [ 1.  0.  0.  0.  0.  1.]</span><br><span class="line"> [ 0.  1.  0.  0.  1.  0.]</span><br><span class="line"> [ 0.  0.  1.  0.  0.  0.]]</span><br></pre></td></tr></table></figure></p>
<h3 id="使用0和1初始化"><a href="#使用0和1初始化" class="headerlink" title="使用0和1初始化"></a>使用0和1初始化</h3><p>现在学习如何初始化一个零和一个向量。需要调用的函数是tf.ones（）。要用零初始化，可以使用tf.zeros（）来代替。这些函数输入一个形状，并分别返回一个同样形状其元素值为0和1的数组。</p>
<p>练习：实现下面的函数来获取形状并返回一个数组（形状的维数）。</p>
<ul>
<li>tf.ones(shape)<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># GRADED FUNCTION: ones</span><br><span class="line"></span><br><span class="line">def ones(shape):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    创建一个值全为1的数组</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    shape -- 数组的形状</span><br><span class="line">        </span><br><span class="line">    Returns: </span><br><span class="line">    ones -- 数组元素仅为1</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    ### 填写代码 ###</span><br><span class="line">    </span><br><span class="line">    # 用tf.ones(...)创建“ones”</span><br><span class="line">    ones = None</span><br><span class="line">    </span><br><span class="line">    # 创建session </span><br><span class="line">    sess = None</span><br><span class="line">    </span><br><span class="line">    # 运行session计算ones</span><br><span class="line">    ones = None</span><br><span class="line">    </span><br><span class="line">    # 关闭session</span><br><span class="line">    None</span><br><span class="line">    </span><br><span class="line">    ### 完成代码 ###</span><br><span class="line">    return ones</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print (&quot;ones = &quot; + str(ones([3])))</span><br></pre></td></tr></table></figure>
<p>此段代码运行结果为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ones = [ 1.  1.  1.]</span><br></pre></td></tr></table></figure></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">NYY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2018/12/06/tensorflow/1.1TensorFlow_basic_exercise_one/">http://yoursite.com/2018/12/06/tensorflow/1.1TensorFlow_basic_exercise_one/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Tensorflow/">Tensorflow</a><a class="post-meta__tags" href="/tags/入门教程/">入门教程</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/12/06/tensorflow/2.1create_neural_network/"><i class="fa fa-chevron-left">  </i><span>TensorFlow搭建神经网络</span></a></div><div class="next-post pull-right"><a href="/2018/12/06/tensorflow/1.TensorFlow入门/"><span>TensorFlow基础入门</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://yoursite.com/2018/12/06/tensorflow/1.1TensorFlow_basic_exercise_one/';
  this.page.identifier = '2018/12/06/tensorflow/1.1TensorFlow_basic_exercise_one/';
  this.page.title = 'TensorFlow基础实验一';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'niuyuanyuan' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-src" src="https://niuyuanyuan.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By NYY</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>