<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="IDT算法论文笔记"><meta name="keywords" content="计算机视觉,Action Recognition,论文笔记"><meta name="author" content="NYY,undefined"><meta name="copyright" content="NYY"><title>IDT算法论文笔记 | NYY's blog</title><link rel="shortcut icon" href="/img/my_icon.jpg"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Action-Recognition-with-Improved-Trajectories论文笔记"><span class="toc-number">1.</span> <span class="toc-text">Action Recognition with Improved Trajectories论文笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#相机运动估计"><span class="toc-number">1.1.</span> <span class="toc-text">相机运动估计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#删除因人类行为导致的不一致匹配"><span class="toc-number">1.2.</span> <span class="toc-text">删除因人类行为导致的不一致匹配</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#轨迹特征"><span class="toc-number">1.3.</span> <span class="toc-text">轨迹特征</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#特征编码"><span class="toc-number">1.4.</span> <span class="toc-text">特征编码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Fisher-Vector"><span class="toc-number">1.4.1.</span> <span class="toc-text">Fisher Vector</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#iDT中的FV"><span class="toc-number">1.4.2.</span> <span class="toc-text">iDT中的FV</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">NYY</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/niuyuanyuanna" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">47</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">34</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://www.ouyangsong.com" target="_blank">欧阳松的博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://github.com/niuyuanyuanna/BlogImages/raw/master/background/computer_version.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">NYY's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">IDT算法论文笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-01-02</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Computer-Version/">Computer Version</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/2019/01/02/computer_version/IDT/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2019/01/02/computer_version/IDT/"></span></a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2,142</span><span class="post-meta__separator">|</span><span>Reading time: 8 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Action-Recognition-with-Improved-Trajectories论文笔记"><a href="#Action-Recognition-with-Improved-Trajectories论文笔记" class="headerlink" title="Action Recognition with Improved Trajectories论文笔记"></a>Action Recognition with Improved Trajectories论文笔记</h1><p>论文原文地址<a href="https://ieeexplore.ieee.org/abstract/document/6751553/" target="_blank" rel="noopener">Action Recognition with Improved Trajectories</a></p>
<p>前面一篇介绍了DT算法，iDT算法在此基础上进行改进，基本框架和DT算法相同，考虑了相机的运动，在相邻帧之间使用SURF特征和密集光流进行特征匹配，引入对背景光流的消除，使得特征更集中于对人的运动描述。特征正则化的方式及特征的编码方式也发生了改变。</p>
<h2 id="相机运动估计"><a href="#相机运动估计" class="headerlink" title="相机运动估计"></a>相机运动估计</h2><p>最重要的一处改进，通过相机运动估计来消除背景光流以及轨迹。首先看DT算法中在没消除背景干扰时的轨迹分布。</p>
<p><div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/IDT.png" width="50%"><br></div><br>由于相机在运动，所以背景上也有很多轨迹，人的轨迹会受到相机运动的影响，而这些信息与要识别的动作关系不大，属于干扰信息，因此希望能够识别并消除这些轨迹。实际上轨迹的运动也是通过计算光流信息进行计算的，因此需要通过估计相机运动，来消除背景区域的光流。</p>
<p>为了检测出相机的背景运动，假设相邻两帧的同形物（homography）是相关的，同形物排除了人类、车辆等独立移动的物体。由于相邻两帧图像之间变化很小，iDT算法假设相邻的两帧图像之间的关系可以用一个投影变换矩阵来描述，即后一帧图像是前一帧图像通过投影变换得到的。因此，估计相机运动的问题就变成了利用前后帧图像计算投影变换矩阵的问题。</p>
<ol>
<li>第一步是检测同形物需要找到连续帧之间的关系，使用了两种方法：</li>
</ol>
<ul>
<li>提取SURF特征并根据最近邻规则匹配；</li>
<li>从光流中采样运动矢量。这里使用基于多项式展开的高效光流法</li>
</ul>
<p>两种方法是互补的，SURF特征主要为了提取斑点类型的特征，光流特征则是为了采集边角特征</p>
<ol start="2">
<li>第二步，获得同形体后，使用RANSAC算法鲁棒C算法估计投影变换矩阵。具体操作为：</li>
</ol>
<p>记$t$时刻和$t+1$时刻的灰度图像分别为$I_t$和$I_{t+1}$,用两张图像计算得到投影变换矩阵$H$（$I_{t+1} = H \times I_t$）。然后用$H$的逆对$I_{t+1}$进行变换（warp），即:</p>
<p>$$<br>I_{t+1}^{wrap} = H^{-1} \times I_{t+1}<br>$$</p>
<p>$I_{t+1}^{wrap}$为假设不存在相机运动时$t+1$时刻的图像；用$I_t$和$I_{t+1}^{wrap}$就可以计算得到优化后的光流。</p>
<p>对于DT来说，消除相机运动有两个好处：</p>
<ul>
<li>对于HOF及MBH特征来说有很大的提升，用单一描述子的分类准确率比起DT中有很大的提高 </li>
<li>移除相机运动产生的轨迹，通过设置阈值，消除优化后的光流中位移矢量的幅值小于阈值的轨迹。</li>
</ul>
<h2 id="删除因人类行为导致的不一致匹配"><a href="#删除因人类行为导致的不一致匹配" class="headerlink" title="删除因人类行为导致的不一致匹配"></a>删除因人类行为导致的不一致匹配</h2><p>图像中人的动作可能比较显著，人身上的匹配点对会使得投影矩阵的估计不准确。因此iDT算法中使用一个huaman detector检测人的位置框，并去除该框中的同形体。从而使得人的运动不影响投影矩阵的估计。iDT中使用的是当时效果最好的human detector，其文章为”Weakly supervised learning of interactions between humans and objects”。</p>
<h2 id="轨迹特征"><a href="#轨迹特征" class="headerlink" title="轨迹特征"></a>轨迹特征</h2><p>对于每个轨迹，都计算四个描述符（T、HOG、HOF、MBH），同DT相同，对特征点进行密集采样和跟踪。在相邻帧之间提取特征匹配，使用RANSAC方法校正homography。还使用human detector进行人体检测，删除包含人体的homography。然后用第$t$帧估计到的$t+1$帧的homography计算得到$I_{t+1}^{wrap}$，在$I_{t+1}^{wrap}$上计算运动描述符（HOF、MBH），HOG保持不变，对每帧图像都进行homography和$I_{t+1}^{wrap}$进行计算以防止误差的传播，得到描述符后，使用RootSIFT方法进行标准化，即进行L1的标准化。</p>
<h2 id="特征编码"><a href="#特征编码" class="headerlink" title="特征编码"></a>特征编码</h2><p>在DT中，使用BoF的方法对得到的描述符进行编码，这篇论文采用FV（Fisher Vector）进行编码。</p>
<h3 id="Fisher-Vector"><a href="#Fisher-Vector" class="headerlink" title="Fisher Vector"></a>Fisher Vector</h3><ol>
<li>对于一幅图像，提取$T$个描述子，每个描述子是$D$维的，可以用$X = { x_t \in \mathbb{R} ^{D} , t = 1,…,T}$表示，假设$T$个描述子符合独立同分布（i.i.d），则有：</li>
</ol>
<p>$$<br>p(X| \lambda) = \prod_{t=1}^{T}p(x_t| \lambda)<br>$$</p>
<p>取对数后可得：</p>
<p>$$<br>\mathfrak{L} (X| \lambda) = \sum _{t =1}^{T} log p(x_t | \lambda)<br>$$</p>
<p>用一组K个高斯分布的线性组合（即GMM混合高斯模型）来逼近这个分布，其参数即为$\lambda$。GMM模型可以用下式描述：</p>
<p>$$<br>p(x_t | \lambda) = \sum _{i=1}^{K} w_ip_i(x_t | \lambda _i)<br>$$</p>
<p>$p_i$为第$i$个基高斯分布：<br>$$<br>p_i(x_t | \lambda) = \frac{1}{(2 \pi)^{\frac{D}{2}} |\sum_i|^{\frac{1}{2}}} \cdot<br>e^{-\frac{(x-\mu_i)’(x-\mu_i)}{2\sum_i}}<br>$$</p>
<p>此处$\lambda$在计算FV时是已知量，是预先通过GMM求解得到的。$\lambda = { \omega_i, \mu_i, \sum_i, i=1,…,K }$为描述独立同分布的参数，其中</p>
<ul>
<li>$\omega_i$为系数，$w_i \geq 0$，$\sum_i \omega_i = 1$；</li>
<li>$\mu_i$为均值</li>
<li>$\sum_i$为标准差</li>
</ul>
<p>在GMM中的目标为：求参数$\lambda$使得$p(X| \lambda)$有最大值，使得它确定的概率分布生成这些给定数据点的概率最大</p>
<ol start="2">
<li>在计算前先定义占有概率，即特征$x_t$由第$i$个高斯分布生成的概率：</li>
</ol>
<p>$$<br>\begin{equation}<br>\begin{aligned}<br>\gamma _t(i)  &amp;= p(i | x_t, \lambda) \<br>&amp;= \frac{\omega_i p_i (x_t| \lambda)}{\sum_{j=1}^K w_j p_j (x_t | \lambda)}<br>\end{aligned}<br>\end{equation}<br>$$</p>
<p>对各参数求偏导：<br>$$<br>\begin{equation}<br>\begin{aligned}<br>&amp;\frac{\partial \mathfrak{L}(X|\lambda)}{\partial \omega_i} = \sum_{t=1}^{T} [\frac{\gamma _t(i)}{\omega_i} - \frac{\gamma _t(1)}{\omega_1}] \quad for(i \geq 2) \<br>&amp;\frac{\partial \mathfrak{L}(X|\lambda)}{\partial \mu_i^d} = \sum_{t=1}^{T}<br>\gamma _t(i)[\frac{x_t^d - \mu_i^d}{(\sigma_i^d)^2}] \<br>&amp;\frac{\partial \mathfrak{L}(X|\lambda)}{\partial \sigma_i^d} = \sum_{t=1}^{T}<br>\gamma _t(i)[\frac{(x_t^d - \mu_i^d)^2}{(\sigma_i^d)^3} - \frac{1}{\sigma_i^d}] \<br>\end{aligned}<br>\end{equation}<br>$$</p>
<p>注意此处的$i$是指第$i$个高斯分布，$d$是指$x_t$的第$d$维，得到的结果数目为 </p>
<ul>
<li>$\omega$：$K-1$个；</li>
<li>均值：$K*D$个；</li>
<li>标准差$K*D$个。</li>
</ul>
<p>因此共有$(2D+1)*K-1$个偏导结果，这里的$-1$是由于$\omega_i$的约束。</p>
<ol start="3">
<li>在计算完之后，还需要进行归一化。对三种变量分别计算归一化需要的fisher matrix的对角线元素的期望： </li>
</ol>
<p>$$<br>\begin{equation}<br>\begin{aligned}<br>&amp; f_{\omega_i} = T(\frac{1}{\omega_i} + \frac{1}{\omega_1}) \<br>&amp; f_{\mu_i^d} = \frac{T_ {\omega _i}}{(\sigma_i^d)^2} \<br>&amp; f_{\sigma_i^d} = \frac{2T_{\omega_i}}{(\sigma_i^d)^2}<br>\end{aligned}<br>\end{equation}<br>$$</p>
<p>$T$为描述子的数目，最终归一化的FV结果为：<br>$$<br>\begin{equation}<br>\begin{aligned}<br>&amp; f_{\omega_i} ^{-\frac{1}{2}} \frac{\partial \mathfrak{L}(X|\lambda)}{\partial \omega_i} \<br>&amp; f_{\mu_i^d} ^{-\frac{1}{2}} \frac{\partial \mathfrak{L}(X|\lambda)}{\partial \mu_i^d} \<br>&amp; f_{\sigma_i^d} ^{-\frac{1}{2}} \frac{\partial \mathfrak{L}(X|\lambda)}{\partial \sigma_i^d}<br>\end{aligned}<br>\end{equation}<br>$$<br>综上，基于Fisher Vector的图像学习的完整过程应该描述为下面几个步骤：</p>
<ol>
<li>选择GMM中K的大小；</li>
<li>用训练图片集中所有的特征（或其子集）来求解GMM（可以用EM方法），得到各个参数；</li>
<li>取待编码的一张图像，求得其特征集合；</li>
<li>用GMM的先验参数以及这张图像的特征集合按照以上步骤求得其fv；</li>
<li>在对训练集中所有图片进行2,3两步的处理后可以获得fishervector的训练集，然后可以用SVM或者其他分类器进行训练。</li>
</ol>
<p>经过fisher vector的编码，提高了图像特征的维度，能够更好的用来描述图像。FisherVector相对于BOF的优势在于，BOF得到的是一个及其稀疏的向量，由于BOF只关注了关键词的数量信息，这是一个0阶的统计信息；Fisher Vector并不稀疏，同时，除了0阶信息，Fisher Vector还包含了1阶(期望)信息、2阶(方差信息)，因此Fisher Vector可以更加充分地表示一幅图片。</p>
<h3 id="iDT中的FV"><a href="#iDT中的FV" class="headerlink" title="iDT中的FV"></a>iDT中的FV</h3><ul>
<li>用于训练的特征长度：trajectory+HOF+HOG+MBH=30+96+108+192=426维</li>
<li>用于训练的特征个数：从训练集中随机采样了256000个</li>
<li><p>PCA降维比例：2，即维度除以2，降维后特征长度为213。先降维，后编码</p>
</li>
<li><p>Fisher Vector中高斯聚类的个数K：K=256</p>
</li>
</ul>
<p>编码完成后使用SVM分类。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">NYY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/01/02/computer_version/IDT/">http://yoursite.com/2019/01/02/computer_version/IDT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/计算机视觉/">计算机视觉</a><a class="post-meta__tags" href="/tags/Action-Recognition/">Action Recognition</a><a class="post-meta__tags" href="/tags/论文笔记/">论文笔记</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/01/03/computer_version/R-C3D/"><i class="fa fa-chevron-left">  </i><span>R-C3D论文笔记</span></a></div><div class="next-post pull-right"><a href="/2018/12/27/computer_version/C3D/"><span>C3D论文笔记</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://yoursite.com/2019/01/02/computer_version/IDT/';
  this.page.identifier = '2019/01/02/computer_version/IDT/';
  this.page.title = 'IDT算法论文笔记';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'niuyuanyuan' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-src" src="https://niuyuanyuan.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By NYY</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>