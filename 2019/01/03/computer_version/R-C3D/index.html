<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="R-C3D论文笔记"><meta name="keywords" content="计算机视觉,Action Recognition,论文笔记"><meta name="author" content="NYY,undefined"><meta name="copyright" content="NYY"><title>R-C3D论文笔记 | NYY's blog</title><link rel="shortcut icon" href="/img/my_icon.jpg"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#R-C3D-Region-Convolutional-3D-Network-for-Temporal-Activity-Detection论文笔记"><span class="toc-number">1.</span> <span class="toc-text">R-C3D: Region Convolutional 3D Network for Temporal Activity Detection论文笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#网络结构"><span class="toc-number">1.1.</span> <span class="toc-text">网络结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#特征提取网络"><span class="toc-number">1.1.1.</span> <span class="toc-text">特征提取网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#候选框子网络Temporal-Proposal-Subnet"><span class="toc-number">1.1.2.</span> <span class="toc-text">候选框子网络Temporal Proposal Subnet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#anchor-segments生成"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">anchor segments生成</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">NYY</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/niuyuanyuanna" target="_blank">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">47</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">34</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">9</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://www.ouyangsong.com" target="_blank">欧阳松的博客</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://github.com/niuyuanyuanna/BlogImages/raw/master/background/computer_version.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">NYY's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">R-C3D论文笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-01-03</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Computer-Version/">Computer Version</a><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="/2019/01/03/computer_version/R-C3D/#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2019/01/03/computer_version/R-C3D/"></span></a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">661</span><span class="post-meta__separator">|</span><span>Reading time: 2 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="R-C3D-Region-Convolutional-3D-Network-for-Temporal-Activity-Detection论文笔记"><a href="#R-C3D-Region-Convolutional-3D-Network-for-Temporal-Activity-Detection论文笔记" class="headerlink" title="R-C3D: Region Convolutional 3D Network for Temporal Activity Detection论文笔记"></a>R-C3D: Region Convolutional 3D Network for Temporal Activity Detection论文笔记</h1><p><a href="https://arxiv.org/abs/1703.07814" target="_blank" rel="noopener">R-C3D: Region Convolutional 3D Network for Temporal Activity Detection</a>发表于ICCV2017，以C3D网络为基础，结合Faster R-CNN物体检测方法，对于任意输入的L帧视频，先用Faster R-CNN找到人体proposal，再进行3D-conv，最后进行回归和分类。文章的主要贡献为：</p>
<ol>
<li>可以对任意长度视频、任意长度行为结合proposal和分类进行端到端的检测；</li>
<li>通过共享Progposal generation 和Classification网络的C3D参数提升网络速度；</li>
<li>在三个测试集上测试并取得了不错的效果。</li>
</ol>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>R-C3D将目标检测方法faster R-CNN的RPN网络应用到人体检测中，并且将PoI pooling推广到3D维度，并进行端到端的训练。其网络结构如下：</p>
<p><div align="center"><br><img src="https://github.com/niuyuanyuanna/BlogImages/raw/master/computerVersion/R-C3D.png" width="100%"><br></div><br>整个网络分为四个部分：</p>
<ol>
<li>特征提取网络：对于输入任意长度的视频进行特征提取</li>
<li>候选框子网络：提取可能存在行为的时序片段</li>
<li>动作分类子网络：对检测到的行为时序进行分类</li>
<li>损失函数</li>
</ol>
<h3 id="特征提取网络"><a href="#特征提取网络" class="headerlink" title="特征提取网络"></a>特征提取网络</h3><p>骨干网络采用C3D网络，输入为一系列RGB的视频帧，输入维度为$\mathbb{R} ^ {3 \times L \times H \times W}$，经过C3D网络的5层卷积网络，输出的特征图维度为$C_{conv5b} \in \mathbb{R} ^{512 \times \frac{L}{8} \times \frac{H}{16} \times \frac{W}{16}}$。此处的输出作为Proposal subnet和Classification subnet的公共注入。$H$和$W$为112，长度$L$试内存情况而定。</p>
<h3 id="候选框子网络Temporal-Proposal-Subnet"><a href="#候选框子网络Temporal-Proposal-Subnet" class="headerlink" title="候选框子网络Temporal Proposal Subnet"></a>候选框子网络Temporal Proposal Subnet</h3><p>为了检测出所有长度的proposal，在候选框子网络中引入faster R-CNN中的RPN网络。</p>
<p>Temporal Proposal Subnet输入为C3D特征，为$C_{conv5b} \in \mathbb{R} ^{512 \times \frac{L}{8} \times \frac{H}{16} \times \frac{W}{16}}$，每个时序上的特征被保存在512维的向量里，对于这个512维的向量，基于anchor segments计算时序偏差。</p>
<h4 id="anchor-segments生成"><a href="#anchor-segments生成" class="headerlink" title="anchor segments生成"></a>anchor segments生成</h4><p>将anchor视频段分类为该anchor视频段是否有activity。anchor视频段初始化为多尺度视频段，均匀分布于以$\frac{L}{8}$为中心的视频段中，每个时间段有$K$个anchor，所以一个$L$长度的视频流共有$\frac{L}{8} * K$个anchor segments。</p>
<p>为了得到anchor segments在每个时间位置的的特征，在$C_{conv5b}$后增加一个$3<em>3</em>3$的3D卷积来扩展Temporal Proposal Subnet的感受野；然后将其从$\frac{H}{16} \times \frac{W}{16}$下采样到$1 \times 1$，生成一个仅针对时间的特征图$C_{tnp} \in \mathbb{R} ^{512 \times \frac{L}{8} \times1 \times 1}$。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">NYY</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/01/03/computer_version/R-C3D/">http://yoursite.com/2019/01/03/computer_version/R-C3D/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/计算机视觉/">计算机视觉</a><a class="post-meta__tags" href="/tags/Action-Recognition/">Action Recognition</a><a class="post-meta__tags" href="/tags/论文笔记/">论文笔记</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/01/22/leetCode/leetcode0122/"><i class="fa fa-chevron-left">  </i><span>Top 100 Liked Questions (1)</span></a></div><div class="next-post pull-right"><a href="/2019/01/02/computer_version/IDT/"><span>IDT算法论文笔记</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'http://yoursite.com/2019/01/03/computer_version/R-C3D/';
  this.page.identifier = '2019/01/03/computer_version/R-C3D/';
  this.page.title = 'R-C3D论文笔记';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'niuyuanyuan' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-src" src="https://niuyuanyuan.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By NYY</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>